#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿæµ‹è¯•è„šæœ¬ - éªŒè¯é¡¹ç›®ç»“æ„æ•´ç†åçš„åŠŸèƒ½
"""

import sys
import os
import time

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

def test_project_structure():
    """æµ‹è¯•é¡¹ç›®ç»“æ„"""
    print("ğŸ” æµ‹è¯•é¡¹ç›®ç»“æ„...")
    
    # æ£€æŸ¥å…³é”®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    src_dir = os.path.join(os.path.dirname(__file__), '..', 'src')
    tests_dir = os.path.dirname(__file__)
    
    key_files = {
        'ä¸»ç¨‹åº': os.path.join(src_dir, 'main_with_tokenizers.py'),
        'è®¡ç®—å¼•æ“': os.path.join(src_dir, 'asr_metrics_refactored.py'),
        'åˆ†è¯å™¨æ¨¡å—': os.path.join(src_dir, 'text_tokenizers', '__init__.py'),
        'HanLPé›†æˆæµ‹è¯•': os.path.join(tests_dir, 'test_hanlp_integration.py'),
        'ç®€å•æµ‹è¯•': os.path.join(tests_dir, 'simple_test.py'),
    }
    
    all_exist = True
    for name, path in key_files.items():
        if os.path.exists(path):
            print(f"   âœ… {name}: å­˜åœ¨")
        else:
            print(f"   âŒ {name}: ç¼ºå¤± ({path})")
            all_exist = False
    
    return all_exist

def test_imports():
    """æµ‹è¯•æ ¸å¿ƒæ¨¡å—å¯¼å…¥"""
    print("\nğŸ“¦ æµ‹è¯•æ ¸å¿ƒæ¨¡å—å¯¼å…¥...")
    
    try:
        from text_tokenizers import get_available_tokenizers, get_tokenizer
        print("   âœ… åˆ†è¯å™¨æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        from asr_metrics_refactored import ASRMetrics
        print("   âœ… ASRåº¦é‡æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        return True
    except Exception as e:
        print(f"   âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False

def test_tokenizers():
    """æµ‹è¯•åˆ†è¯å™¨åŠŸèƒ½"""
    print("\nğŸ”¤ æµ‹è¯•åˆ†è¯å™¨åŠŸèƒ½...")
    
    try:
        from text_tokenizers import get_available_tokenizers, get_tokenizer
        
        # è·å–å¯ç”¨åˆ†è¯å™¨
        tokenizers = get_available_tokenizers()
        print(f"   å¯ç”¨åˆ†è¯å™¨: {tokenizers}")
        
        # æµ‹è¯•æ¯ä¸ªåˆ†è¯å™¨
        test_text = "æµ‹è¯•ä¸­æ–‡åˆ†è¯åŠŸèƒ½"
        results = {}
        
        for tokenizer_name in tokenizers:
            try:
                start_time = time.time()
                tokenizer = get_tokenizer(tokenizer_name)
                words = tokenizer.cut(test_text)
                end_time = time.time()
                
                results[tokenizer_name] = {
                    'words': words,
                    'time': end_time - start_time,
                    'status': 'âœ…'
                }
                print(f"   âœ… {tokenizer_name}: {words} (è€—æ—¶: {end_time - start_time:.4f}ç§’)")
                
            except Exception as e:
                results[tokenizer_name] = {
                    'error': str(e),
                    'status': 'âŒ'
                }
                print(f"   âŒ {tokenizer_name}: å¤±è´¥ - {e}")
        
        # æ£€æŸ¥HanLPæ˜¯å¦å¯ç”¨
        hanlp_available = 'hanlp' in results and results['hanlp']['status'] == 'âœ…'
        if hanlp_available:
            print("   ğŸ‰ HanLPåˆ†è¯å™¨å·¥ä½œæ­£å¸¸ï¼")
        
        return len(results) > 0 and any(r['status'] == 'âœ…' for r in results.values())
        
    except Exception as e:
        print(f"   âŒ åˆ†è¯å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_asr_metrics():
    """æµ‹è¯•ASRåº¦é‡è®¡ç®—"""
    print("\nğŸ“Š æµ‹è¯•ASRåº¦é‡è®¡ç®—...")
    
    try:
        from asr_metrics_refactored import ASRMetrics
        from text_tokenizers import get_available_tokenizers
        
        tokenizers = get_available_tokenizers()
        if not tokenizers:
            print("   âš ï¸ æ²¡æœ‰å¯ç”¨çš„åˆ†è¯å™¨")
            return False
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„åˆ†è¯å™¨è¿›è¡Œæµ‹è¯•
        tokenizer_name = tokenizers[0]
        print(f"   ä½¿ç”¨åˆ†è¯å™¨: {tokenizer_name}")
        
        asr_metrics = ASRMetrics(tokenizer_name=tokenizer_name)
        
        # æµ‹è¯•CERè®¡ç®—
        reference = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­"
        hypothesis = "è¿™æ˜¯ä¸ªæµ‹è¯•å¥å­"
        
        cer = asr_metrics.calculate_cer(reference, hypothesis)
        print(f"   CERè®¡ç®—ç»“æœ: {cer}")
        
        # å¦‚æœHanLPå¯ç”¨ï¼Œä¹Ÿæµ‹è¯•ä¸€ä¸‹
        if 'hanlp' in tokenizers:
            print("   ğŸ¯ ä½¿ç”¨HanLPè¿›è¡Œé«˜ç²¾åº¦æµ‹è¯•...")
            hanlp_metrics = ASRMetrics(tokenizer_name='hanlp')
            hanlp_cer = hanlp_metrics.calculate_cer(reference, hypothesis)
            print(f"   HanLP CERç»“æœ: {hanlp_cer}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ ASRåº¦é‡æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ ASRå­—å‡†ç»Ÿè®¡å·¥å…· - é¡¹ç›®ç»“æ„æ•´ç†åå¿«é€Ÿæµ‹è¯•")
    print("=" * 60)
    
    tests = [
        ("é¡¹ç›®ç»“æ„", test_project_structure),
        ("æ¨¡å—å¯¼å…¥", test_imports), 
        ("åˆ†è¯å™¨åŠŸèƒ½", test_tokenizers),
        ("ASRåº¦é‡è®¡ç®—", test_asr_metrics)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        try:
            if test_func():
                passed += 1
                print(f"âœ… {test_name}: é€šè¿‡")
            else:
                print(f"âŒ {test_name}: å¤±è´¥")
        except Exception as e:
            print(f"âŒ {test_name}: å¼‚å¸¸ - {e}")
        
        print("-" * 40)
    
    print("=" * 60)
    print(f"ğŸ“ˆ æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼é¡¹ç›®ç»“æ„æ•´ç†æˆåŠŸï¼ŒåŠŸèƒ½æ­£å¸¸ï¼")
        print("ğŸ’¡ å¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨ä¸»ç¨‹åº:")
        print("   cd ../src && python main_with_tokenizers.py")
    elif passed >= total // 2:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•é€šè¿‡ï¼Œå»ºè®®æ£€æŸ¥å¤±è´¥çš„é¡¹ç›®")
    else:
        print("âŒ å¤šé¡¹æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦ä¿®å¤é—®é¢˜")
    
    print("=" * 60)
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 