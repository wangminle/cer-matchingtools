#!/usr/bin/env python3
"""
æµ‹è¯•thulacå’Œhanlpåˆ†è¯å™¨æ˜¯å¦ä½¿ç”¨äº†ç›¸åŒçš„æ¨¡å‹
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from text_tokenizers.tokenizers.factory import TokenizerFactory

def test_tokenizer_models():
    """æµ‹è¯•åˆ†è¯å™¨æ˜¯å¦ä½¿ç”¨äº†ä¸åŒçš„æ¨¡å‹"""
    
    print("ğŸ” æµ‹è¯•åˆ†è¯å™¨æ¨¡å‹ç‹¬ç«‹æ€§...")
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨",
        "ä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦",
        "äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•",
        "ä»Šå¤©å¤©æ°”å¾ˆå¥½"
    ]
    
    try:
        # è·å–åˆ†è¯å™¨å®ä¾‹
        thulac_tokenizer = TokenizerFactory.get_tokenizer('thulac')
        hanlp_tokenizer = TokenizerFactory.get_tokenizer('hanlp')
        
        print(f"\nğŸ“Š åˆ†è¯å™¨ä¿¡æ¯:")
        print(f"THULACå®ä¾‹ID: {id(thulac_tokenizer)}")
        print(f"HanLPå®ä¾‹ID: {id(hanlp_tokenizer)}")
        print(f"THULACæ¨¡å‹: {type(thulac_tokenizer.thu) if hasattr(thulac_tokenizer, 'thu') else 'None'}")
        print(f"HanLPæ¨¡å‹: {type(hanlp_tokenizer.tok_model) if hasattr(hanlp_tokenizer, 'tok_model') else 'None'}")
        
        # æ£€æŸ¥æ¨¡å‹å¯¹è±¡æ˜¯å¦ç›¸åŒ
        if hasattr(thulac_tokenizer, 'thu') and hasattr(hanlp_tokenizer, 'tok_model'):
            print(f"\nğŸ” æ¨¡å‹å¯¹è±¡æ£€æŸ¥:")
            print(f"THULACæ¨¡å‹ID: {id(thulac_tokenizer.thu)}")
            print(f"HanLPæ¨¡å‹ID: {id(hanlp_tokenizer.tok_model)}")
            print(f"æ¨¡å‹æ˜¯å¦ç›¸åŒ: {thulac_tokenizer.thu is hanlp_tokenizer.tok_model}")
        
        # æµ‹è¯•åˆ†è¯ç»“æœ
        print(f"\nğŸ“ åˆ†è¯ç»“æœå¯¹æ¯”:")
        results_identical = True
        
        for text in test_texts:
            thulac_result = thulac_tokenizer.cut(text)
            hanlp_result = hanlp_tokenizer.cut(text)
            
            print(f"\næ–‡æœ¬: {text}")
            print(f"THULAC: {thulac_result}")
            print(f"HanLP:  {hanlp_result}")
            print(f"ç»“æœç›¸åŒ: {thulac_result == hanlp_result}")
            
            if thulac_result != hanlp_result:
                results_identical = False
        
        print(f"\nğŸ¯ æµ‹è¯•ç»“è®º:")
        if results_identical:
            print("âš ï¸  è­¦å‘Šï¼šæ‰€æœ‰æµ‹è¯•æ–‡æœ¬çš„åˆ†è¯ç»“æœéƒ½ç›¸åŒï¼")
            print("   è¿™å¯èƒ½è¡¨æ˜ä¸¤ä¸ªåˆ†è¯å™¨ä½¿ç”¨äº†ç›¸åŒçš„æ¨¡å‹æˆ–ç®—æ³•")
        else:
            print("âœ… æ­£å¸¸ï¼šä¸¤ä¸ªåˆ†è¯å™¨äº§ç”Ÿäº†ä¸åŒçš„ç»“æœ")
            
        # æ£€æŸ¥ç¼“å­˜çŠ¶æ€
        print(f"\nğŸ’¾ ç¼“å­˜çŠ¶æ€:")
        cache = TokenizerFactory._tokenizers
        print(f"ç¼“å­˜çš„åˆ†è¯å™¨: {list(cache.keys())}")
        for name, tokenizer in cache.items():
            print(f"{name}: {id(tokenizer)}")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

def test_separate_instances():
    """æµ‹è¯•åˆ›å»ºç‹¬ç«‹å®ä¾‹æ˜¯å¦è§£å†³é—®é¢˜"""
    
    print("\nğŸ”„ æµ‹è¯•ç‹¬ç«‹å®ä¾‹åˆ›å»º...")
    
    try:
        # æ¸…é™¤ç¼“å­˜
        TokenizerFactory.clear_cache()
        
        # åˆ›å»ºç‹¬ç«‹å®ä¾‹
        thulac1 = TokenizerFactory.create_tokenizer('thulac')
        thulac2 = TokenizerFactory.create_tokenizer('thulac')
        hanlp1 = TokenizerFactory.create_tokenizer('hanlp')
        hanlp2 = TokenizerFactory.create_tokenizer('hanlp')
        
        print(f"THULACå®ä¾‹1 ID: {id(thulac1)}")
        print(f"THULACå®ä¾‹2 ID: {id(thulac2)}")
        print(f"HanLPå®ä¾‹1 ID:  {id(hanlp1)}")
        print(f"HanLPå®ä¾‹2 ID:  {id(hanlp2)}")
        
        print(f"THULACå®ä¾‹æ˜¯å¦ç›¸åŒ: {thulac1 is thulac2}")
        print(f"HanLPå®ä¾‹æ˜¯å¦ç›¸åŒ: {hanlp1 is hanlp2}")
        
        # æµ‹è¯•åˆ†è¯ç»“æœ
        test_text = "æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨"
        thulac1_result = thulac1.cut(test_text)
        hanlp1_result = hanlp1.cut(test_text)
        
        print(f"\næµ‹è¯•æ–‡æœ¬: {test_text}")
        print(f"THULACç»“æœ: {thulac1_result}")
        print(f"HanLPç»“æœ:  {hanlp1_result}")
        print(f"ç»“æœç›¸åŒ: {thulac1_result == hanlp1_result}")
        
    except Exception as e:
        print(f"âŒ ç‹¬ç«‹å®ä¾‹æµ‹è¯•å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    test_tokenizer_models()
    test_separate_instances() 