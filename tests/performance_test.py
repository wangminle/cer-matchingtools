#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•ä¼˜åŒ–å‰åçš„æ€§èƒ½å·®å¼‚
"""

import sys
import os
# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../dev/src'))

import time
from asr_metrics_refactored import ASRMetrics
from text_tokenizers import get_available_tokenizers
from text_tokenizers.tokenizers.factory import TokenizerFactory

def performance_test():
    """æ€§èƒ½æµ‹è¯•ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ASRåˆ†è¯å™¨æ€§èƒ½æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•æ•°æ®
    test_cases = [
        ("çŸ­æ–‡æœ¬", "ä»Šå¤©å¤©æ°”å¾ˆå¥½", "ä»Šå¤©å¤©æ°”å¾ˆå·"),
        ("ä¸­ç­‰æ–‡æœ¬", "æˆ‘æ¥åˆ°åŒ—äº¬æ¸…åå¤§å­¦å­¦ä¹ è®¡ç®—æœºç§‘å­¦æŠ€æœ¯", "æˆ‘æ¥åˆ°åŒ—äº¬æ¸…åå¤§å­¦å­¦ä¹ è®¡ç®—æœºç§‘å­¦æŠ€"),
        ("é•¿æ–‡æœ¬", 
         "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨", 
         "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯å®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨å¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨")
    ]
    
    available_tokenizers = get_available_tokenizers()
    print(f"å¯ç”¨åˆ†è¯å™¨: {available_tokenizers}")
    
    for tokenizer_name in available_tokenizers:
        print(f"\n{'='*40}")
        print(f"æµ‹è¯•åˆ†è¯å™¨: {tokenizer_name}")
        print(f"{'='*40}")
        
        # æµ‹è¯•åˆå§‹åŒ–æ—¶é—´
        start_time = time.time()
        metrics = ASRMetrics(tokenizer_name=tokenizer_name)
        init_time = time.time() - start_time
        print(f"åˆå§‹åŒ–æ—¶é—´: {init_time:.4f} ç§’")
        
        # æµ‹è¯•è®¡ç®—æ—¶é—´
        for test_name, ref_text, hyp_text in test_cases:
            print(f"\n--- {test_name} ---")
            
            # æµ‹è¯•CERè®¡ç®—æ—¶é—´
            start_time = time.time()
            cer = metrics.calculate_cer(ref_text, hyp_text)
            cer_time = time.time() - start_time
            print(f"CERè®¡ç®—æ—¶é—´: {cer_time:.4f} ç§’, ç»“æœ: {cer:.4f}")
            
            # æµ‹è¯•è¯¦ç»†æŒ‡æ ‡è®¡ç®—æ—¶é—´
            start_time = time.time()
            detailed = metrics.calculate_detailed_metrics(ref_text, hyp_text)
            detailed_time = time.time() - start_time
            print(f"è¯¦ç»†æŒ‡æ ‡æ—¶é—´: {detailed_time:.4f} ç§’")
            
            # æµ‹è¯•è¯­æ°”è¯è¿‡æ»¤åŠŸèƒ½
            start_time = time.time()
            cer_filtered = metrics.calculate_cer(ref_text, hyp_text, filter_fillers=True)
            filtered_time = time.time() - start_time
            print(f"è¯­æ°”è¯è¿‡æ»¤æ—¶é—´: {filtered_time:.4f} ç§’, ç»“æœ: {cer_filtered:.4f}")
    
    print(f"\n{'='*60}")
    print("ç¼“å­˜æµ‹è¯• - é‡å¤åˆå§‹åŒ–åŒä¸€åˆ†è¯å™¨")
    print(f"{'='*60}")
    
    # æµ‹è¯•ç¼“å­˜æ•ˆæœ - é‡å¤åˆ›å»ºåŒä¸€ä¸ªåˆ†è¯å™¨
    for tokenizer_name in available_tokenizers[:1]:  # åªæµ‹è¯•ç¬¬ä¸€ä¸ª
        print(f"\næµ‹è¯• {tokenizer_name} é‡å¤åˆå§‹åŒ–:")
        
        # ç¬¬ä¸€æ¬¡åˆå§‹åŒ–
        start_time = time.time()
        metrics1 = ASRMetrics(tokenizer_name=tokenizer_name)
        first_init_time = time.time() - start_time
        print(f"ç¬¬1æ¬¡åˆå§‹åŒ–: {first_init_time:.4f} ç§’")
        
        # ç¬¬äºŒæ¬¡åˆå§‹åŒ–ï¼ˆåº”è¯¥ä½¿ç”¨ç¼“å­˜ï¼‰
        start_time = time.time()
        metrics2 = ASRMetrics(tokenizer_name=tokenizer_name)
        second_init_time = time.time() - start_time
        print(f"ç¬¬2æ¬¡åˆå§‹åŒ–: {second_init_time:.4f} ç§’")
        
        # ç¬¬ä¸‰æ¬¡åˆå§‹åŒ–ï¼ˆåº”è¯¥ä½¿ç”¨ç¼“å­˜ï¼‰
        start_time = time.time()
        metrics3 = ASRMetrics(tokenizer_name=tokenizer_name)
        third_init_time = time.time() - start_time
        print(f"ç¬¬3æ¬¡åˆå§‹åŒ–: {third_init_time:.4f} ç§’")
        
        if first_init_time > 0:
            print(f"ç¼“å­˜æ•ˆæœ: ç¬¬2æ¬¡æ¯”ç¬¬1æ¬¡å¿« {((first_init_time - second_init_time) / first_init_time * 100):.1f}%")
            print(f"ç¼“å­˜æ•ˆæœ: ç¬¬3æ¬¡æ¯”ç¬¬1æ¬¡å¿« {((first_init_time - third_init_time) / first_init_time * 100):.1f}%")
        else:
            print("ç¼“å­˜æ•ˆæœ: åˆå§‹åŒ–æ—¶é—´å¤ªçŸ­ï¼Œæ— æ³•å‡†ç¡®æµ‹é‡å·®å¼‚")

    print(f"\n{'='*60}")
    print("å·¥å‚ç¼“å­˜æµ‹è¯•")
    print(f"{'='*60}")
    
    # æµ‹è¯•å·¥å‚ç¼“å­˜
    start_time = time.time()
    available1 = get_available_tokenizers()
    first_call_time = time.time() - start_time
    print(f"ç¬¬1æ¬¡è°ƒç”¨ get_available_tokenizers(): {first_call_time:.4f} ç§’")
    
    start_time = time.time()
    available2 = get_available_tokenizers()
    second_call_time = time.time() - start_time
    print(f"ç¬¬2æ¬¡è°ƒç”¨ get_available_tokenizers(): {second_call_time:.4f} ç§’")
    
    if first_call_time > 0:
        print(f"ç¼“å­˜æ•ˆæœ: ç¬¬2æ¬¡æ¯”ç¬¬1æ¬¡å¿« {((first_call_time - second_call_time) / first_call_time * 100):.1f}%")
    
    print(f"\nğŸ‰ æ€§èƒ½æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    performance_test() 