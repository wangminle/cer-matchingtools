#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†è¯å™¨æ¶æ„æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯åˆ†è¯å™¨æ¨¡å—æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import sys
import os

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../dev/src'))

def test_tokenizers():
    """æµ‹è¯•åˆ†è¯å™¨æ¶æ„"""
    print("=" * 60)
    print("åˆ†è¯å™¨æ¶æ„æµ‹è¯•")
    print("=" * 60)
    
    try:
        # å¯¼å…¥åˆ†è¯å™¨æ¨¡å—
        from text_tokenizers import get_available_tokenizers, get_tokenizer_info, get_tokenizer
        
        print("âœ“ åˆ†è¯å™¨æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # è·å–å¯ç”¨çš„åˆ†è¯å™¨åˆ—è¡¨
        available_tokenizers = get_available_tokenizers()
        print(f"\nå¯ç”¨çš„åˆ†è¯å™¨: {available_tokenizers}")
        
        if not available_tokenizers:
            print("âš ï¸  è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„åˆ†è¯å™¨")
            return False
        
        # æµ‹è¯•æ¯ä¸ªå¯ç”¨çš„åˆ†è¯å™¨
        test_text = "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œæˆ‘ä»¬å»å…¬å›­æ•£æ­¥å§ã€‚"
        
        for tokenizer_name in available_tokenizers:
            print(f"\n{'='*40}")
            print(f"æµ‹è¯•åˆ†è¯å™¨: {tokenizer_name}")
            print(f"{'='*40}")
            
            # è·å–åˆ†è¯å™¨ä¿¡æ¯
            try:
                info = get_tokenizer_info(tokenizer_name)
                print(f"åˆ†è¯å™¨ä¿¡æ¯:")
                print(f"  - åç§°: {info.get('name', 'N/A')}")
                print(f"  - ç‰ˆæœ¬: {info.get('version', 'N/A')}")
                print(f"  - å¯ç”¨æ€§: {'å¯ç”¨' if info.get('available', False) else 'ä¸å¯ç”¨'}")
                print(f"  - æè¿°: {info.get('description', 'N/A')}")
                
                if not info.get('available', False):
                    print(f"  - é”™è¯¯: {info.get('error', 'N/A')}")
                    continue
                    
            except Exception as e:
                print(f"âœ— è·å–åˆ†è¯å™¨ä¿¡æ¯å¤±è´¥: {str(e)}")
                continue
            
            # æµ‹è¯•åˆ†è¯å™¨åŠŸèƒ½
            try:
                tokenizer = get_tokenizer(tokenizer_name)
                print(f"\nâœ“ {tokenizer_name} åˆ†è¯å™¨åˆ›å»ºæˆåŠŸ")
                
                # æµ‹è¯•åŸºç¡€åˆ†è¯
                try:
                    words = tokenizer.cut(test_text)
                    print(f"åŸºç¡€åˆ†è¯ç»“æœ: {words}")
                except Exception as e:
                    print(f"âœ— åŸºç¡€åˆ†è¯å¤±è´¥: {str(e)}")
                
                # æµ‹è¯•è¯æ€§æ ‡æ³¨
                try:
                    pos_result = tokenizer.posseg(test_text)
                    print(f"è¯æ€§æ ‡æ³¨ç»“æœ: {pos_result[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ª
                except Exception as e:
                    print(f"âœ— è¯æ€§æ ‡æ³¨å¤±è´¥: {str(e)}")
                
                # æµ‹è¯•ç²¾ç¡®åˆ†è¯
                try:
                    tokenize_result = tokenizer.tokenize(test_text)
                    print(f"ç²¾ç¡®åˆ†è¯ç»“æœ: {tokenize_result[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ª
                except Exception as e:
                    print(f"âœ— ç²¾ç¡®åˆ†è¯å¤±è´¥: {str(e)}")
                
            except Exception as e:
                print(f"âœ— {tokenizer_name} åˆ†è¯å™¨æµ‹è¯•å¤±è´¥: {str(e)}")
        
        return True
        
    except ImportError as e:
        print(f"âœ— åˆ†è¯å™¨æ¨¡å—å¯¼å…¥å¤±è´¥: {str(e)}")
        return False
    except Exception as e:
        print(f"âœ— æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°æœªçŸ¥é”™è¯¯: {str(e)}")
        return False


def test_asr_metrics():
    """æµ‹è¯•é‡æ„åçš„ASRMetricsç±»"""
    print("\n" + "=" * 60)
    print("ASRMetricsç±»æµ‹è¯•")
    print("=" * 60)
    
    try:
        from asr_metrics_refactored import ASRMetrics
        print("âœ“ ASRMetricsç±»å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•æ•°æ®
        reference_text = "ä»Šå¤©å¤©æ°”å¾ˆå¥½"
        hypothesis_text = "ä»Šå¤©å¤©æ°”å¾ˆå·"
        
        # è·å–å¯ç”¨çš„åˆ†è¯å™¨
        from text_tokenizers import get_available_tokenizers
        available_tokenizers = get_available_tokenizers()
        
        if not available_tokenizers:
            print("âš ï¸  è­¦å‘Š: æ²¡æœ‰å¯ç”¨çš„åˆ†è¯å™¨è¿›è¡Œæµ‹è¯•")
            return False
        
        # æµ‹è¯•æ¯ä¸ªåˆ†è¯å™¨
        for tokenizer_name in available_tokenizers:
            print(f"\nä½¿ç”¨åˆ†è¯å™¨: {tokenizer_name}")
            print("-" * 40)
            
            try:
                # åˆ›å»ºASRMetricså®ä¾‹
                metrics = ASRMetrics(tokenizer_name=tokenizer_name)
                print(f"âœ“ ä½¿ç”¨{tokenizer_name}åˆ›å»ºASRMetricsæˆåŠŸ")
                
                # è®¡ç®—CER
                cer = metrics.calculate_cer(reference_text, hypothesis_text)
                print(f"å­—ç¬¦é”™è¯¯ç‡ (CER): {cer:.4f}")
                
                # è®¡ç®—å‡†ç¡®ç‡
                accuracy = metrics.calculate_accuracy(reference_text, hypothesis_text)
                print(f"å‡†ç¡®ç‡: {accuracy:.4f}")
                
                # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
                detailed_metrics = metrics.calculate_detailed_metrics(reference_text, hypothesis_text)
                print(f"è¯¦ç»†æŒ‡æ ‡:")
                print(f"  - å‚è€ƒæ–‡æœ¬é•¿åº¦: {detailed_metrics['ref_length']}")
                print(f"  - å‡è®¾æ–‡æœ¬é•¿åº¦: {detailed_metrics['hyp_length']}")
                print(f"  - æ›¿æ¢é”™è¯¯: {detailed_metrics['substitutions']}")
                print(f"  - åˆ é™¤é”™è¯¯: {detailed_metrics['deletions']}")
                print(f"  - æ’å…¥é”™è¯¯: {detailed_metrics['insertions']}")
                print(f"  - ä½¿ç”¨åˆ†è¯å™¨: {detailed_metrics['tokenizer']}")
                
                # æµ‹è¯•è¯­æ°”è¯è¿‡æ»¤
                cer_filtered = metrics.calculate_cer(reference_text, hypothesis_text, filter_fillers=True)
                print(f"è¯­æ°”è¯è¿‡æ»¤åCER: {cer_filtered:.4f}")
                
            except Exception as e:
                print(f"âœ— ä½¿ç”¨{tokenizer_name}æµ‹è¯•ASRMetricså¤±è´¥: {str(e)}")
        
        return True
        
    except ImportError as e:
        print(f"âœ— ASRMetricsç±»å¯¼å…¥å¤±è´¥: {str(e)}")
        return False
    except Exception as e:
        print(f"âœ— ASRMetricsæµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹åˆ†è¯å™¨æ¶æ„æµ‹è¯•...")
    
    # æµ‹è¯•åˆ†è¯å™¨æ¶æ„
    tokenizer_test_result = test_tokenizers()
    
    # æµ‹è¯•ASRMetricsç±»
    asr_metrics_test_result = test_asr_metrics()
    
    # æ€»ç»“æµ‹è¯•ç»“æœ
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ€»ç»“")
    print("=" * 60)
    
    if tokenizer_test_result:
        print("âœ“ åˆ†è¯å™¨æ¶æ„æµ‹è¯•: é€šè¿‡")
    else:
        print("âœ— åˆ†è¯å™¨æ¶æ„æµ‹è¯•: å¤±è´¥")
    
    if asr_metrics_test_result:
        print("âœ“ ASRMetricsç±»æµ‹è¯•: é€šè¿‡")
    else:
        print("âœ— ASRMetricsç±»æµ‹è¯•: å¤±è´¥")
    
    if tokenizer_test_result and asr_metrics_test_result:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼åˆ†è¯å™¨æ¶æ„å·¥ä½œæ­£å¸¸ã€‚")
        return True
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³é—®é¢˜ã€‚")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 