#!/usr/bin/env python3
"""
è¯¦ç»†çš„åˆ†è¯å™¨å¯¹æ¯”æµ‹è¯• - ä½¿ç”¨æ›´å¤æ‚çš„æµ‹è¯•ç”¨ä¾‹
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from text_tokenizers.tokenizers.factory import TokenizerFactory

def comprehensive_tokenizer_test():
    """å…¨é¢çš„åˆ†è¯å™¨æµ‹è¯•"""
    
    print("ğŸ” å…¨é¢åˆ†è¯å™¨å¯¹æ¯”æµ‹è¯•...")
    
    # æ›´å¤æ‚å’Œå¤šæ ·åŒ–çš„æµ‹è¯•æ–‡æœ¬
    complex_test_texts = [
        # åŸºç¡€æµ‹è¯•
        "æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨",
        "ä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦",
        
        # æ­§ä¹‰è¯æµ‹è¯•
        "ç ”ç©¶ç”Ÿå‘½çš„èµ·æº",  # "ç ”ç©¶ç”Ÿ"è¿˜æ˜¯"ç ”ç©¶"+"ç”Ÿå‘½"
        "ä¹’ä¹“çƒæ‹å–å®Œäº†",  # "ä¹’ä¹“çƒæ‹"è¿˜æ˜¯"ä¹’ä¹“çƒ"+"æ‹å–"
        "ä»–è¯´çš„ç¡®å®åœ¨ç†",  # "çš„ç¡®"è¿˜æ˜¯"çš„"+"ç¡®å®"
        
        # ä¸“æœ‰åè¯æµ‹è¯•
        "æ¸…åå¤§å­¦è®¡ç®—æœºç³»",
        "ä¸­å›½äººæ°‘è§£æ”¾å†›",
        "é˜¿é‡Œå·´å·´é›†å›¢",
        
        # æŠ€æœ¯è¯æ±‡æµ‹è¯•
        "æœºå™¨å­¦ä¹ ç®—æ³•ä¼˜åŒ–",
        "æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹",
        "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯",
        
        # æ··åˆæ–‡æœ¬æµ‹è¯•
        "iPhone15 Pro Maxä»·æ ¼ä¸Šæ¶¨äº†",
        "COVID-19ç–«è‹—æ¥ç§å·¥ä½œ",
        "AI+åŒ»ç–—çš„å‘å±•å‰æ™¯",
        
        # å¤æ–‡/è¯—è¯æµ‹è¯•
        "æ˜¥çœ ä¸è§‰æ™“ï¼Œå¤„å¤„é—»å•¼é¸Ÿ",
        "æµ·å†…å­˜çŸ¥å·±ï¼Œå¤©æ¶¯è‹¥æ¯”é‚»",
        
        # é•¿å¥å­æµ‹è¯•
        "åœ¨äººå·¥æ™ºèƒ½å¿«é€Ÿå‘å±•çš„ä»Šå¤©ï¼Œæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨å„ä¸ªé¢†åŸŸéƒ½æœ‰ç€å¹¿æ³›çš„åº”ç”¨",
        
        # æ ‡ç‚¹ç¬¦å·æµ‹è¯•
        "ä½ å¥½ï¼è¯·é—®ç°åœ¨å‡ ç‚¹äº†ï¼Ÿ",
        "è‹¹æœã€é¦™è•‰ã€æ©˜å­éƒ½æ˜¯æ°´æœã€‚",
        
        # æ•°å­—å’Œå•ä½æµ‹è¯•
        "ä»Šå¤©æ°”æ¸©25æ‘„æ°åº¦",
        "è¿™æœ¬ä¹¦å”®ä»·29.9å…ƒ",
        "è·ç¦»ç›®æ ‡è¿˜æœ‰3.5å…¬é‡Œ"
    ]
    
    try:
        # è·å–åˆ†è¯å™¨å®ä¾‹
        thulac_tokenizer = TokenizerFactory.get_tokenizer('thulac')
        hanlp_tokenizer = TokenizerFactory.get_tokenizer('hanlp')
        
        print(f"\nğŸ“Š åˆ†è¯å™¨ä¿¡æ¯:")
        print(f"THULAC: {type(thulac_tokenizer)} (ID: {id(thulac_tokenizer)})")
        print(f"HanLP:  {type(hanlp_tokenizer)} (ID: {id(hanlp_tokenizer)})")
        
        # ç»Ÿè®¡å·®å¼‚
        total_tests = len(complex_test_texts)
        identical_results = 0
        different_results = 0
        
        print(f"\nğŸ“ è¯¦ç»†åˆ†è¯ç»“æœå¯¹æ¯”:")
        print("=" * 80)
        
        for i, text in enumerate(complex_test_texts, 1):
            thulac_result = thulac_tokenizer.cut(text)
            hanlp_result = hanlp_tokenizer.cut(text)
            
            is_identical = thulac_result == hanlp_result
            if is_identical:
                identical_results += 1
            else:
                different_results += 1
            
            # æ˜¾ç¤ºç»“æœ
            status = "âœ… ç›¸åŒ" if is_identical else "âŒ ä¸åŒ"
            print(f"\n{i:2d}. æ–‡æœ¬: {text}")
            print(f"    THULAC: {thulac_result}")
            print(f"    HanLP:  {hanlp_result}")
            print(f"    çŠ¶æ€:   {status}")
            
            # å¦‚æœä¸åŒï¼Œè¯¦ç»†åˆ†æå·®å¼‚
            if not is_identical:
                print(f"    ğŸ“Š å·®å¼‚åˆ†æ:")
                print(f"       THULACè¯æ•°: {len(thulac_result)}")
                print(f"       HanLPè¯æ•°:  {len(hanlp_result)}")
                
                # æ‰¾å‡ºä¸åŒçš„è¯
                thulac_set = set(thulac_result)
                hanlp_set = set(hanlp_result)
                thulac_only = thulac_set - hanlp_set
                hanlp_only = hanlp_set - thulac_set
                
                if thulac_only:
                    print(f"       THULACç‹¬æœ‰: {list(thulac_only)}")
                if hanlp_only:
                    print(f"       HanLPç‹¬æœ‰:  {list(hanlp_only)}")
        
        print("\n" + "=" * 80)
        print(f"ğŸ¯ æµ‹è¯•ç»Ÿè®¡:")
        print(f"   æ€»æµ‹è¯•æ•°:     {total_tests}")
        print(f"   ç»“æœç›¸åŒ:     {identical_results} ({identical_results/total_tests*100:.1f}%)")
        print(f"   ç»“æœä¸åŒ:     {different_results} ({different_results/total_tests*100:.1f}%)")
        
        if identical_results == total_tests:
            print(f"\nâš ï¸  ä¸¥é‡é—®é¢˜: æ‰€æœ‰{total_tests}ä¸ªæµ‹è¯•æ–‡æœ¬çš„åˆ†è¯ç»“æœéƒ½å®Œå…¨ç›¸åŒï¼")
            print("   è¿™è¡¨æ˜å¯èƒ½å­˜åœ¨ä»¥ä¸‹é—®é¢˜ä¹‹ä¸€:")
            print("   1. HanLPæ¨¡å‹å®é™…æœªç”Ÿæ•ˆï¼Œä½¿ç”¨äº†fallbackæœºåˆ¶")
            print("   2. ä¸¤ä¸ªåˆ†è¯å™¨åº•å±‚ä½¿ç”¨äº†ç›¸åŒçš„åˆ†è¯é€»è¾‘")
            print("   3. å­˜åœ¨æŸç§ç¼“å­˜æˆ–å•ä¾‹æ¨¡å¼é—®é¢˜")
        elif identical_results > total_tests * 0.8:
            print(f"\nâš ï¸  å¯ç–‘: {identical_results/total_tests*100:.1f}%çš„ç»“æœç›¸åŒï¼Œæ¯”ä¾‹è¿‡é«˜")
        else:
            print(f"\nâœ… æ­£å¸¸: åˆ†è¯å™¨è¡¨ç°å‡ºäº†é¢„æœŸçš„å·®å¼‚æ€§")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

def test_model_independence():
    """æµ‹è¯•æ¨¡å‹ç‹¬ç«‹æ€§"""
    
    print(f"\nğŸ”¬ æµ‹è¯•æ¨¡å‹ç‹¬ç«‹æ€§...")
    
    try:
        # ç›´æ¥ä½¿ç”¨HanLPå’ŒTHULACåº“è¿›è¡Œæµ‹è¯•
        import hanlp
        import thulac
        
        # ç›´æ¥åŠ è½½æ¨¡å‹
        hanlp_model = hanlp.load(hanlp.pretrained.tok.SIGHAN2005_PKU_BERT_BASE_ZH)
        thulac_model = thulac.thulac(seg_only=True)
        
        test_text = "ç ”ç©¶ç”Ÿå‘½çš„èµ·æº"
        
        # ç›´æ¥ä½¿ç”¨æ¨¡å‹
        hanlp_direct = hanlp_model(test_text)
        thulac_direct_raw = thulac_model.cut(test_text, text=True)
        thulac_direct = thulac_direct_raw.split()
        
        print(f"æµ‹è¯•æ–‡æœ¬: {test_text}")
        print(f"HanLPç›´æ¥è°ƒç”¨: {hanlp_direct}")
        print(f"THULACç›´æ¥è°ƒç”¨: {thulac_direct}")
        print(f"ç›´æ¥è°ƒç”¨ç»“æœç›¸åŒ: {hanlp_direct == thulac_direct}")
        
        # ä¸å°è£…ç‰ˆæœ¬å¯¹æ¯”
        thulac_tokenizer = TokenizerFactory.get_tokenizer('thulac')
        hanlp_tokenizer = TokenizerFactory.get_tokenizer('hanlp')
        
        thulac_wrapped = thulac_tokenizer.cut(test_text)
        hanlp_wrapped = hanlp_tokenizer.cut(test_text)
        
        print(f"\nTHULACå°è£…ç‰ˆæœ¬: {thulac_wrapped}")
        print(f"HanLPå°è£…ç‰ˆæœ¬:  {hanlp_wrapped}")
        print(f"å°è£…ç‰ˆæœ¬ç»“æœç›¸åŒ: {thulac_wrapped == hanlp_wrapped}")
        
        print(f"\nğŸ” ä¸€è‡´æ€§æ£€æŸ¥:")
        print(f"THULACç›´æ¥vså°è£…: {thulac_direct == thulac_wrapped}")
        print(f"HanLPç›´æ¥vså°è£…:  {hanlp_direct == hanlp_wrapped}")
        
    except Exception as e:
        print(f"âŒ ç‹¬ç«‹æ€§æµ‹è¯•å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    comprehensive_tokenizer_test()
    test_model_independence() 