#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†è¯å™¨æ¶æ„æ¼”ç¤º
åˆ›å»ºæ¨¡æ‹Ÿç‰ˆæœ¬æ¥å±•ç¤ºæ¶æ„è®¾è®¡ï¼Œä¸ä¾èµ–å¤–éƒ¨åº“
"""

import sys
import os
from typing import List, Tuple, Dict, Any
from abc import ABC, abstractmethod

# æ¨¡æ‹Ÿåˆ†è¯å™¨å¼‚å¸¸ç±»
class TokenizerError(Exception):
    """åˆ†è¯å™¨åŸºç¡€å¼‚å¸¸ç±»"""
    pass

class TokenizerInitError(TokenizerError):
    """åˆ†è¯å™¨åˆå§‹åŒ–å¼‚å¸¸"""
    pass

class TokenizerProcessError(TokenizerError):
    """åˆ†è¯å™¨å¤„ç†å¼‚å¸¸"""
    pass

# æ¨¡æ‹ŸæŠ½è±¡åŸºç±»
class BaseTokenizer(ABC):
    """åˆ†è¯å™¨æŠ½è±¡åŸºç±»"""
    
    def __init__(self):
        self.name = self.__class__.__name__.replace('Tokenizer', '').lower()
        self.is_initialized = False
        self.version = "demo-1.0"
    
    @abstractmethod
    def initialize(self) -> bool:
        pass
    
    @abstractmethod
    def cut(self, text: str) -> List[str]:
        pass
    
    @abstractmethod
    def posseg(self, text: str) -> List[Tuple[str, str]]:
        pass
    
    @abstractmethod
    def tokenize(self, text: str) -> List[Tuple[str, int, int]]:
        pass
    
    def validate_text(self, text: str) -> str:
        if not isinstance(text, str):
            raise TokenizerProcessError("è¾“å…¥å¿…é¡»æ˜¯å­—ç¬¦ä¸²ç±»å‹")
        return text.strip()
    
    def get_info(self) -> Dict[str, Any]:
        return {
            'name': self.name,
            'version': self.version,
            'initialized': self.is_initialized,
            'available': True,
            'class_name': self.__class__.__name__
        }

# æ¨¡æ‹ŸJiebaåˆ†è¯å™¨
class MockJiebaTokenizer(BaseTokenizer):
    """æ¨¡æ‹ŸJiebaåˆ†è¯å™¨ï¼Œç”¨äºæ¼”ç¤ºæ¶æ„"""
    
    def __init__(self):
        super().__init__()
        self.name = "jieba"
    
    def initialize(self) -> bool:
        try:
            # æ¨¡æ‹Ÿåˆå§‹åŒ–è¿‡ç¨‹
            print("  æ­£åœ¨åˆå§‹åŒ–Jiebaåˆ†è¯å™¨...")
            self.is_initialized = True
            return True
        except Exception as e:
            raise TokenizerInitError(f"Jiebaåˆ†è¯å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
    def cut(self, text: str) -> List[str]:
        cleaned_text = self.validate_text(text)
        if not cleaned_text:
            return []
        
        # ç®€å•çš„ä¸­æ–‡å­—ç¬¦åˆ†å‰²æ¨¡æ‹Ÿï¼ˆä»…ç”¨äºæ¼”ç¤ºï¼‰
        import re
        
        # åˆ†å‰²ä¸­æ–‡å­—ç¬¦ã€è‹±æ–‡å•è¯å’Œæ•°å­—
        pattern = r'[\u4e00-\u9fff]+|[a-zA-Z]+|\d+'
        words = re.findall(pattern, cleaned_text)
        
        # è¿›ä¸€æ­¥åˆ†å‰²ä¸­æ–‡å­—ç¬¦ï¼ˆæ¯2-3ä¸ªå­—ç¬¦ä½œä¸ºä¸€ä¸ªè¯ï¼‰
        result = []
        for word in words:
            if re.match(r'[\u4e00-\u9fff]+', word):
                # ä¸­æ–‡å­—ç¬¦ï¼ŒæŒ‰2-3ä¸ªå­—ç¬¦åˆ†ç»„
                i = 0
                while i < len(word):
                    if i + 2 < len(word):
                        result.append(word[i:i+2])
                        i += 2
                    else:
                        result.append(word[i:])
                        break
            else:
                result.append(word)
        
        return result
    
    def posseg(self, text: str) -> List[Tuple[str, str]]:
        words = self.cut(text)
        # ç®€å•çš„è¯æ€§æ ‡æ³¨æ¨¡æ‹Ÿ
        result = []
        for word in words:
            if word.isdigit():
                pos = 'm'  # æ•°è¯
            elif word.isalpha():
                pos = 'n'  # åè¯
            else:
                pos = 'x'  # å…¶ä»–
            result.append((word, pos))
        return result
    
    def tokenize(self, text: str) -> List[Tuple[str, int, int]]:
        cleaned_text = self.validate_text(text)
        if not cleaned_text:
            return []
        
        words = self.cut(cleaned_text)
        result = []
        current_pos = 0
        
        for word in words:
            start_pos = cleaned_text.find(word, current_pos)
            if start_pos == -1:
                start_pos = current_pos
            end_pos = start_pos + len(word)
            result.append((word, start_pos, end_pos))
            current_pos = end_pos
        
        return result
    
    def get_info(self) -> dict:
        info = super().get_info()
        info.update({
            'description': 'æ¨¡æ‹ŸJiebaä¸­æ–‡åˆ†è¯å™¨',
            'features': ['åˆ†è¯', 'è¯æ€§æ ‡æ³¨', 'ç²¾ç¡®ä½ç½®åˆ†è¯'],
            'dependencies': ['jieba (æ¨¡æ‹Ÿ)'],
            'performance': 'é«˜é€Ÿ',
            'accuracy': 'ä¸­ç­‰'
        })
        return info

# æ¨¡æ‹ŸTHULACåˆ†è¯å™¨
class MockThulacTokenizer(BaseTokenizer):
    """æ¨¡æ‹ŸTHULACåˆ†è¯å™¨ï¼Œç”¨äºæ¼”ç¤ºæ¶æ„"""
    
    def __init__(self):
        super().__init__()
        self.name = "thulac"
    
    def initialize(self) -> bool:
        try:
            print("  æ­£åœ¨åˆå§‹åŒ–THULACåˆ†è¯å™¨...")
            # æ¨¡æ‹Ÿè¾ƒæ…¢çš„åˆå§‹åŒ–è¿‡ç¨‹
            import time
            time.sleep(0.5)
            self.is_initialized = True
            return True
        except Exception as e:
            raise TokenizerInitError(f"THULACåˆ†è¯å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
    def cut(self, text: str) -> List[str]:
        cleaned_text = self.validate_text(text)
        if not cleaned_text:
            return []
        
        # æ¨¡æ‹ŸTHULACçš„åˆ†è¯ç»“æœï¼ˆæ›´ç»†ç²’åº¦ï¼‰
        import re
        
        result = []
        # æŒ‰å•ä¸ªä¸­æ–‡å­—ç¬¦åˆ†å‰²
        for char in cleaned_text:
            if re.match(r'[\u4e00-\u9fff]', char):
                result.append(char)
            elif char.isalnum():
                result.append(char)
        
        return result
    
    def posseg(self, text: str) -> List[Tuple[str, str]]:
        words = self.cut(text)
        # æ¨¡æ‹ŸTHULACçš„è¯æ€§æ ‡æ³¨
        result = []
        for word in words:
            if word.isdigit():
                pos = 'CD'  # æ•°è¯
            elif word.isalpha():
                pos = 'NN'  # åè¯
            else:
                pos = 'PU'  # æ ‡ç‚¹
            result.append((word, pos))
        return result
    
    def tokenize(self, text: str) -> List[Tuple[str, int, int]]:
        cleaned_text = self.validate_text(text)
        words = self.cut(cleaned_text)
        
        result = []
        pos = 0
        for word in words:
            start_pos = pos
            end_pos = pos + len(word)
            result.append((word, start_pos, end_pos))
            pos = end_pos
        
        return result
    
    def get_info(self) -> dict:
        info = super().get_info()
        info.update({
            'description': 'æ¨¡æ‹ŸTHULACé«˜ç²¾åº¦ä¸­æ–‡åˆ†è¯å™¨',
            'features': ['åˆ†è¯', 'è¯æ€§æ ‡æ³¨', 'ç²¾ç¡®ä½ç½®åˆ†è¯'],
            'dependencies': ['thulac (æ¨¡æ‹Ÿ)'],
            'performance': 'ä¸­ç­‰é€Ÿåº¦',
            'accuracy': 'é«˜ç²¾åº¦'
        })
        return info

# æ¨¡æ‹ŸHanLPåˆ†è¯å™¨ï¼ˆä¸å¯ç”¨çŠ¶æ€ï¼‰
class MockHanlpTokenizer(BaseTokenizer):
    """æ¨¡æ‹ŸHanLPåˆ†è¯å™¨ï¼Œæ¼”ç¤ºä¸å¯ç”¨çŠ¶æ€"""
    
    def __init__(self):
        super().__init__()
        self.name = "hanlp"
    
    def initialize(self) -> bool:
        # æ¨¡æ‹Ÿåˆå§‹åŒ–å¤±è´¥
        raise TokenizerInitError("HanLPåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install hanlp")
    
    def cut(self, text: str) -> List[str]:
        raise TokenizerProcessError("HanLPåˆ†è¯å™¨æœªåˆå§‹åŒ–")
    
    def posseg(self, text: str) -> List[Tuple[str, str]]:
        raise TokenizerProcessError("HanLPåˆ†è¯å™¨æœªåˆå§‹åŒ–")
    
    def tokenize(self, text: str) -> List[Tuple[str, int, int]]:
        raise TokenizerProcessError("HanLPåˆ†è¯å™¨æœªåˆå§‹åŒ–")
    
    def get_info(self) -> dict:
        info = super().get_info()
        info.update({
            'description': 'æ¨¡æ‹ŸHanLPæ·±åº¦å­¦ä¹ ä¸­æ–‡åˆ†è¯å™¨',
            'features': ['åˆ†è¯', 'è¯æ€§æ ‡æ³¨', 'ç²¾ç¡®ä½ç½®åˆ†è¯', 'BERTæ”¯æŒ'],
            'dependencies': ['hanlp (æ¨¡æ‹Ÿ)'],
            'performance': 'è¾ƒæ…¢ï¼ˆæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼‰',
            'accuracy': 'æœ€é«˜ç²¾åº¦',
            'available': False,
            'error': 'HanLPåº“æœªå®‰è£…'
        })
        return info

# æ¨¡æ‹Ÿåˆ†è¯å™¨å·¥å‚
class MockTokenizerFactory:
    """æ¨¡æ‹Ÿåˆ†è¯å™¨å·¥å‚ç±»"""
    
    _instance = None
    _tokenizers = {}
    _available_tokenizers = {
        'jieba': MockJiebaTokenizer,
        'thulac': MockThulacTokenizer,
        'hanlp': MockHanlpTokenizer
    }
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(MockTokenizerFactory, cls).__new__(cls)
        return cls._instance
    
    @classmethod
    def get_available_tokenizers(cls) -> List[str]:
        available = []
        for name, tokenizer_class in cls._available_tokenizers.items():
            try:
                tokenizer = tokenizer_class()
                tokenizer.initialize()
                available.append(name)
            except:
                continue
        return available
    
    @classmethod
    def get_tokenizer(cls, name: str) -> BaseTokenizer:
        if name not in cls._available_tokenizers:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†è¯å™¨: {name}")
        
        if name in cls._tokenizers:
            return cls._tokenizers[name]
        
        tokenizer_class = cls._available_tokenizers[name]
        tokenizer = tokenizer_class()
        tokenizer.initialize()
        cls._tokenizers[name] = tokenizer
        return tokenizer
    
    @classmethod
    def get_tokenizer_info(cls, name: str) -> Dict[str, Any]:
        if name not in cls._available_tokenizers:
            return {'name': name, 'available': False, 'error': f'ä¸æ”¯æŒçš„åˆ†è¯å™¨: {name}'}
        
        try:
            tokenizer_class = cls._available_tokenizers[name]
            tokenizer = tokenizer_class()
            tokenizer.initialize()
            info = tokenizer.get_info()
            info['available'] = True
            return info
        except Exception as e:
            return {
                'name': name,
                'available': False,
                'error': str(e)
            }

# æ¨¡æ‹ŸASRMetricsç±»
class MockASRMetrics:
    """æ¨¡æ‹ŸASRMetricsç±»ï¼Œæ¼”ç¤ºåˆ†è¯å™¨é›†æˆ"""
    
    def __init__(self, tokenizer_name: str = "jieba"):
        self.tokenizer_name = tokenizer_name
        self.tokenizer = None
        self._initialize_tokenizer()
    
    def _initialize_tokenizer(self):
        try:
            self.tokenizer = MockTokenizerFactory.get_tokenizer(self.tokenizer_name)
        except Exception as e:
            print(f"è­¦å‘Š: {self.tokenizer_name}åˆ†è¯å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            print("è‡ªåŠ¨å›é€€åˆ°jiebaåˆ†è¯å™¨")
            try:
                self.tokenizer_name = "jieba"
                self.tokenizer = MockTokenizerFactory.get_tokenizer("jieba")
            except Exception as fallback_e:
                raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–ä»»ä½•åˆ†è¯å™¨: {str(fallback_e)}")
    
    def calculate_cer(self, reference: str, hypothesis: str) -> float:
        """ç®€åŒ–çš„CERè®¡ç®—"""
        if not reference or not hypothesis:
            return 1.0 if reference != hypothesis else 0.0
        
        # ä½¿ç”¨å½“å‰åˆ†è¯å™¨é¢„å¤„ç†æ–‡æœ¬
        ref_processed = "".join(self.tokenizer.cut(reference))
        hyp_processed = "".join(self.tokenizer.cut(hypothesis))
        
        # ç®€åŒ–çš„ç¼–è¾‘è·ç¦»è®¡ç®—
        if ref_processed == hyp_processed:
            return 0.0
        
        # æ¨¡æ‹Ÿç¼–è¾‘è·ç¦»è®¡ç®—
        max_len = max(len(ref_processed), len(hyp_processed))
        if max_len == 0:
            return 0.0
        
        # ç®€å•çš„å­—ç¬¦å·®å¼‚è®¡ç®—
        differences = sum(1 for a, b in zip(ref_processed, hyp_processed) if a != b)
        differences += abs(len(ref_processed) - len(hyp_processed))
        
        return min(differences / len(reference), 1.0)
    
    def get_tokenizer_info(self) -> Dict[str, Any]:
        return self.tokenizer.get_info() if self.tokenizer else {}

def demo_tokenizer_architecture():
    """æ¼”ç¤ºåˆ†è¯å™¨æ¶æ„"""
    print("=" * 60)
    print("åˆ†è¯å™¨æ¶æ„æ¼”ç¤º")
    print("=" * 60)
    
    # æµ‹è¯•åˆ†è¯å™¨å·¥å‚
    factory = MockTokenizerFactory()
    
    print("\n1. è·å–å¯ç”¨åˆ†è¯å™¨åˆ—è¡¨:")
    available_tokenizers = factory.get_available_tokenizers()
    print(f"å¯ç”¨åˆ†è¯å™¨: {available_tokenizers}")
    
    print("\n2. æµ‹è¯•å„ä¸ªåˆ†è¯å™¨:")
    test_text = "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œæˆ‘ä»¬å»å…¬å›­æ•£æ­¥ã€‚"
    
    for tokenizer_name in ['jieba', 'thulac', 'hanlp']:
        print(f"\n{'='*40}")
        print(f"æµ‹è¯•åˆ†è¯å™¨: {tokenizer_name}")
        print(f"{'='*40}")
        
        # è·å–åˆ†è¯å™¨ä¿¡æ¯
        info = factory.get_tokenizer_info(tokenizer_name)
        print(f"åˆ†è¯å™¨ä¿¡æ¯:")
        print(f"  åç§°: {info.get('name', 'N/A')}")
        print(f"  å¯ç”¨æ€§: {'å¯ç”¨' if info.get('available', False) else 'ä¸å¯ç”¨'}")
        print(f"  æè¿°: {info.get('description', 'N/A')}")
        
        if not info.get('available', False):
            print(f"  é”™è¯¯: {info.get('error', 'N/A')}")
            continue
        
        # æµ‹è¯•åˆ†è¯å™¨åŠŸèƒ½
        try:
            tokenizer = factory.get_tokenizer(tokenizer_name)
            
            # æµ‹è¯•åŸºç¡€åˆ†è¯
            words = tokenizer.cut(test_text)
            print(f"åŸºç¡€åˆ†è¯ç»“æœ: {words[:5]}...")
            
            # æµ‹è¯•è¯æ€§æ ‡æ³¨
            pos_result = tokenizer.posseg(test_text)
            print(f"è¯æ€§æ ‡æ³¨ç»“æœ: {pos_result[:3]}...")
            
            # æµ‹è¯•ç²¾ç¡®åˆ†è¯
            tokenize_result = tokenizer.tokenize(test_text)
            print(f"ç²¾ç¡®åˆ†è¯ç»“æœ: {tokenize_result[:3]}...")
            
        except Exception as e:
            print(f"æµ‹è¯•å¤±è´¥: {str(e)}")
    
    print("\n3. æµ‹è¯•ASRMetricsé›†æˆ:")
    print("-" * 40)
    
    # æµ‹è¯•æ•°æ®
    reference = "ä»Šå¤©å¤©æ°”å¾ˆå¥½"
    hypothesis = "ä»Šå¤©å¤©æ°”å¾ˆå·"
    
    for tokenizer_name in available_tokenizers:
        try:
            metrics = MockASRMetrics(tokenizer_name=tokenizer_name)
            cer = metrics.calculate_cer(reference, hypothesis)
            print(f"ä½¿ç”¨{tokenizer_name}åˆ†è¯å™¨çš„CER: {cer:.4f}")
        except Exception as e:
            print(f"ä½¿ç”¨{tokenizer_name}åˆ†è¯å™¨æµ‹è¯•å¤±è´¥: {str(e)}")

def demo_gui_integration():
    """æ¼”ç¤ºGUIé›†æˆç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("GUIé›†æˆæ¼”ç¤º")
    print("=" * 60)
    
    print("\næ¨¡æ‹ŸGUIæ“ä½œåºåˆ—:")
    print("1. åº”ç”¨å¯åŠ¨ -> åˆå§‹åŒ–å¯ç”¨åˆ†è¯å™¨åˆ—è¡¨")
    
    factory = MockTokenizerFactory()
    available = factory.get_available_tokenizers()
    print(f"   å¯ç”¨åˆ†è¯å™¨: {available}")
    
    print("\n2. ç”¨æˆ·é€‰æ‹©åˆ†è¯å™¨ -> æ›´æ–°çŠ¶æ€æ˜¾ç¤º")
    for tokenizer in ['jieba', 'thulac', 'hanlp']:
        info = factory.get_tokenizer_info(tokenizer)
        status = "âœ“" if info.get('available', False) else "âœ—"
        print(f"   {status} {tokenizer} - {info.get('description', 'N/A')}")
    
    print("\n3. ç”¨æˆ·å¼€å§‹è®¡ç®— -> ä½¿ç”¨é€‰å®šåˆ†è¯å™¨")
    selected_tokenizer = "jieba"
    print(f"   é€‰å®šåˆ†è¯å™¨: {selected_tokenizer}")
    
    try:
        metrics = MockASRMetrics(tokenizer_name=selected_tokenizer)
        print(f"   âœ“ ASRMetricså®ä¾‹åˆ›å»ºæˆåŠŸ")
        print(f"   ä½¿ç”¨åˆ†è¯å™¨: {metrics.tokenizer_name}")
    except Exception as e:
        print(f"   âœ— åˆ›å»ºå¤±è´¥: {str(e)}")

def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("åˆ†è¯å™¨æ¶æ„æ¼”ç¤ºç¨‹åº")
    print("æ­¤æ¼”ç¤ºä¸ä¾èµ–å¤–éƒ¨åº“ï¼Œå±•ç¤ºå®Œæ•´æ¶æ„è®¾è®¡")
    
    # æ¼”ç¤ºåˆ†è¯å™¨æ¶æ„
    demo_tokenizer_architecture()
    
    # æ¼”ç¤ºGUIé›†æˆ
    demo_gui_integration()
    
    print("\n" + "=" * 60)
    print("æ¼”ç¤ºæ€»ç»“")
    print("=" * 60)
    print("âœ“ åˆ†è¯å™¨æŠ½è±¡æ¶æ„: ç»Ÿä¸€æ¥å£è®¾è®¡")
    print("âœ“ å·¥å‚æ¨¡å¼: åŠ¨æ€åˆ†è¯å™¨ç®¡ç†")
    print("âœ“ å•ä¾‹æ¨¡å¼: é¿å…é‡å¤åˆå§‹åŒ–")
    print("âœ“ é”™è¯¯å¤„ç†: ä¼˜é›…é™çº§æœºåˆ¶")
    print("âœ“ ASRMetricsé›†æˆ: æ”¯æŒå¯åˆ‡æ¢åˆ†è¯å™¨")
    print("âœ“ GUIé›†æˆ: ç”¨æˆ·å‹å¥½çš„åˆ†è¯å™¨é€‰æ‹©")
    print("\nğŸ‰ åˆ†è¯å™¨æ¶æ„è®¾è®¡å®Œæˆï¼")

if __name__ == "__main__":
    main() 