#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•åˆ†è¯å™¨ç‰ˆæœ¬è·å–ä¿®å¤
"""

import sys
import os

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from text_tokenizers import get_available_tokenizers, get_tokenizer_info
from text_tokenizers.tokenizers.factory import TokenizerFactory

def test_tokenizer_versions():
    """æµ‹è¯•æ‰€æœ‰å¯ç”¨åˆ†è¯å™¨çš„ç‰ˆæœ¬è·å–"""
    print("ğŸ”§ æµ‹è¯•åˆ†è¯å™¨ç‰ˆæœ¬è·å–ä¿®å¤æ•ˆæœ")
    print("=" * 50)
    
    # æ¸…é™¤ç°æœ‰ç¼“å­˜
    print("æ¸…é™¤ç°æœ‰ç¼“å­˜...")
    TokenizerFactory.clear_cache()
    
    # è·å–å¯ç”¨åˆ†è¯å™¨
    available_tokenizers = get_available_tokenizers()
    print(f"å¯ç”¨åˆ†è¯å™¨: {available_tokenizers}")
    print()
    
    for tokenizer_name in available_tokenizers:
        print(f"ğŸ“‹ æµ‹è¯• {tokenizer_name} åˆ†è¯å™¨:")
        try:
            info = get_tokenizer_info(tokenizer_name)
            
            print(f"  åˆ†è¯å™¨åç§°: {info.get('name', 'N/A')}")
            print(f"  ç‰ˆæœ¬: {info.get('version', 'N/A')}")
            print(f"  åˆå§‹åŒ–çŠ¶æ€: {info.get('initialized', False)}")
            print(f"  å¯ç”¨æ€§: {info.get('available', False)}")
            
            if info.get('version', 'unknown') == 'unknown':
                print(f"  âš ï¸  ç‰ˆæœ¬è·å–å¤±è´¥")
            else:
                print(f"  âœ… ç‰ˆæœ¬è·å–æˆåŠŸ")
                
        except Exception as e:
            print(f"  âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        
        print()

if __name__ == "__main__":
    test_tokenizer_versions() 