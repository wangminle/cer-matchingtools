"""
THULACåˆ†è¯å™¨å®ç°
åŸºäºTHULACåº“çš„åˆ†è¯å™¨ï¼Œæä¾›é«˜ç²¾åº¦ä¸­æ–‡åˆ†è¯
"""

from typing import List, Tuple
from .base import BaseTokenizer, TokenizerInitError, TokenizerProcessError


class ThulacTokenizer(BaseTokenizer):
    """
    THULACåˆ†è¯å™¨å®ç°
    åŸºäºTHULACåº“ï¼Œæä¾›é«˜ç²¾åº¦ä¸­æ–‡åˆ†è¯å’Œè¯æ€§æ ‡æ³¨åŠŸèƒ½
    """
    
    def __init__(self):
        super().__init__()
        self.name = "thulac"
        self.thu = None
    
    def initialize(self) -> bool:
        """
        åˆå§‹åŒ–THULACåˆ†è¯å™¨
        
        Returns:
            bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
            
        Raises:
            TokenizerInitError: åˆå§‹åŒ–å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            # æ£€æŸ¥THULACæ˜¯å¦å¯ç”¨
            try:
                import thulac
            except ImportError:
                raise TokenizerInitError("THULACåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install thulac")
            
            # åˆå§‹åŒ–THULACå®ä¾‹
            try:
                self.thu = thulac.thulac(seg_only=False)  # seg_only=Falseè¡¨ç¤ºåŒæ—¶è¿›è¡Œåˆ†è¯å’Œè¯æ€§æ ‡æ³¨
                
                # æµ‹è¯•THULACæ˜¯å¦æ­£å¸¸å·¥ä½œ
                test_result = self.thu.cut("æµ‹è¯•", text=True)
                if not test_result:
                    raise TokenizerInitError("THULACåˆ†è¯å™¨æµ‹è¯•å¤±è´¥")
                
                # è·å–ç‰ˆæœ¬ä¿¡æ¯
                try:
                    # ğŸ”§ ä¿®å¤: ä½¿ç”¨importlib.metadataè·å–ç‰ˆæœ¬ä¿¡æ¯ï¼ˆPython 3.8+æ¨èæ–¹å¼ï¼‰
                    try:
                        from importlib.metadata import version
                        self.version = version('thulac')
                    except ImportError:
                        # Python < 3.8 ä½¿ç”¨importlib_metadata
                        try:
                            from importlib_metadata import version
                            self.version = version('thulac')
                        except ImportError:
                            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨pkg_resources
                            try:
                                import pkg_resources
                                self.version = pkg_resources.get_distribution('thulac').version
                            except:
                                # æœ€åå¤‡ç”¨æ–¹æ¡ˆï¼šå°è¯•ä»æ¨¡å—å±æ€§è·å–
                                self.version = getattr(thulac, '__version__', '0.2.x')
                    except Exception as e:
                        print(f"è·å–THULACç‰ˆæœ¬å¤±è´¥: {str(e)}")
                        self.version = '0.2.x'  # è®¾ç½®é»˜è®¤ç‰ˆæœ¬
                except Exception:
                    self.version = 'unknown'
                
                self.is_initialized = True
                return True
                
            except Exception as e:
                raise TokenizerInitError(f"THULACåˆå§‹åŒ–å¤±è´¥: {str(e)}")
                
        except Exception as e:
            raise TokenizerInitError(f"THULACåˆ†è¯å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
    def cut(self, text: str) -> List[str]:
        """
        åŸºç¡€åˆ†è¯åŠŸèƒ½
        
        Args:
            text (str): å¾…åˆ†è¯çš„æ–‡æœ¬
            
        Returns:
            List[str]: åˆ†è¯ç»“æœåˆ—è¡¨
            
        Raises:
            TokenizerProcessError: åˆ†è¯å¤„ç†å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            if not self.is_initialized:
                raise TokenizerProcessError("THULACåˆ†è¯å™¨æœªåˆå§‹åŒ–")
            
            cleaned_text = self.validate_text(text)
            if not cleaned_text:
                return []
            
            # ä½¿ç”¨THULACè¿›è¡Œåˆ†è¯ï¼Œåªè¿”å›è¯è¯­
            result_text = self.thu.cut(cleaned_text, text=True)
            # åˆ†å‰²æˆè¯è¯­åˆ—è¡¨
            words = result_text.split()
            
            # æå–è¯è¯­éƒ¨åˆ†ï¼ˆå»æ‰è¯æ€§æ ‡æ³¨ï¼‰
            result = []
            for word in words:
                if '_' in word:
                    word_part = word.split('_')[0]
                    result.append(word_part)
                else:
                    result.append(word)
            
            return result
            
        except Exception as e:
            raise TokenizerProcessError(f"THULACåˆ†è¯å¤±è´¥: {str(e)}")
    
    def posseg(self, text: str) -> List[Tuple[str, str]]:
        """
        è¯æ€§æ ‡æ³¨åŠŸèƒ½
        
        Args:
            text (str): å¾…æ ‡æ³¨çš„æ–‡æœ¬
            
        Returns:
            List[Tuple[str, str]]: (è¯è¯­, è¯æ€§)çš„å…ƒç»„åˆ—è¡¨
            
        Raises:
            TokenizerProcessError: è¯æ€§æ ‡æ³¨å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            if not self.is_initialized:
                raise TokenizerProcessError("THULACåˆ†è¯å™¨æœªåˆå§‹åŒ–")
            
            cleaned_text = self.validate_text(text)
            if not cleaned_text:
                return []
            
            # ä½¿ç”¨THULACè¿›è¡Œè¯æ€§æ ‡æ³¨
            result_text = self.thu.cut(cleaned_text, text=True)
            # åˆ†å‰²æˆè¯è¯­åˆ—è¡¨
            words = result_text.split()
            
            # è§£æè¯è¯­å’Œè¯æ€§
            result = []
            for word_pos in words:
                if '_' in word_pos:
                    word, pos = word_pos.split('_', 1)
                    result.append((word, pos))
                else:
                    # å¦‚æœæ²¡æœ‰è¯æ€§æ ‡æ³¨ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    result.append((word_pos, 'unk'))
            
            return result
            
        except Exception as e:
            raise TokenizerProcessError(f"THULACè¯æ€§æ ‡æ³¨å¤±è´¥: {str(e)}")
    
    def tokenize(self, text: str) -> List[Tuple[str, int, int]]:
        """
        ç²¾ç¡®åˆ†è¯ï¼Œè¿”å›è¯è¯­åŠå…¶åœ¨åŸæ–‡ä¸­çš„ä½ç½®
        ç”±äºTHULACä¸ç›´æ¥æä¾›ä½ç½®ä¿¡æ¯ï¼Œéœ€è¦æ‰‹åŠ¨è®¡ç®—
        
        Args:
            text (str): å¾…åˆ†è¯çš„æ–‡æœ¬
            
        Returns:
            List[Tuple[str, int, int]]: (è¯è¯­, å¼€å§‹ä½ç½®, ç»“æŸä½ç½®)çš„å…ƒç»„åˆ—è¡¨
            
        Raises:
            TokenizerProcessError: åˆ†è¯å¤„ç†å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            if not self.is_initialized:
                raise TokenizerProcessError("THULACåˆ†è¯å™¨æœªåˆå§‹åŒ–")
            
            cleaned_text = self.validate_text(text)
            if not cleaned_text:
                return []
            
            # è·å–åˆ†è¯ç»“æœ
            words = self.cut(cleaned_text)
            
            # æ‰‹åŠ¨è®¡ç®—ä½ç½®ä¿¡æ¯
            result = []
            current_pos = 0
            text_chars = list(cleaned_text)
            
            for word in words:
                word_chars = list(word)
                word_len = len(word_chars)
                
                # åœ¨å‰©ä½™æ–‡æœ¬ä¸­æŸ¥æ‰¾å½“å‰è¯è¯­
                start_pos = current_pos
                found = False
                
                # å‘å‰æœç´¢åŒ¹é…ä½ç½®
                while start_pos <= len(text_chars) - word_len:
                    # æ£€æŸ¥æ˜¯å¦åŒ¹é…
                    match = True
                    for i, char in enumerate(word_chars):
                        if start_pos + i >= len(text_chars) or text_chars[start_pos + i] != char:
                            match = False
                            break
                    
                    if match:
                        end_pos = start_pos + word_len
                        result.append((word, start_pos, end_pos))
                        current_pos = end_pos
                        found = True
                        break
                    
                    start_pos += 1
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…ï¼ˆå¯èƒ½ç”±äºåˆ†è¯ç»“æœä¸åŸæ–‡ä¸ä¸€è‡´ï¼‰ï¼Œä½¿ç”¨è¿‘ä¼¼ä½ç½®
                if not found:
                    end_pos = current_pos + word_len
                    if end_pos > len(text_chars):
                        end_pos = len(text_chars)
                    result.append((word, current_pos, end_pos))
                    current_pos = end_pos
            
            return result
            
        except Exception as e:
            raise TokenizerProcessError(f"THULACç²¾ç¡®åˆ†è¯å¤±è´¥: {str(e)}")
    
    def get_info(self) -> dict:
        """
        è·å–THULACåˆ†è¯å™¨ä¿¡æ¯
        
        Returns:
            dict: åŒ…å«åˆ†è¯å™¨è¯¦ç»†ä¿¡æ¯çš„å­—å…¸
        """
        info = super().get_info()
        info.update({
            'description': 'åŸºäºTHULACåº“çš„ä¸­æ–‡åˆ†è¯å™¨',
            'features': ['åˆ†è¯', 'è¯æ€§æ ‡æ³¨', 'ç²¾ç¡®ä½ç½®åˆ†è¯'],
            'dependencies': ['thulac'],
            'performance': 'ä¸­ç­‰',
            'accuracy': 'é«˜ç²¾åº¦',
            'note': 'tokenizeæ–¹æ³•ä½¿ç”¨æ‰‹åŠ¨ä½ç½®è®¡ç®—ï¼Œå¯èƒ½å­˜åœ¨ç²¾åº¦å·®å¼‚'
        })
        return info 