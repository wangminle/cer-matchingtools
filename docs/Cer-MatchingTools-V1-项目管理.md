# ASR字准确率对比工具 - 项目管理文档

## 项目概述
本项目是一个用于批量对比ASR（自动语音识别）转写结果与标准文本之间字准确率的Python工具，支持GUI界面、命令行和示例测试三种使用方式。

## 已实现功能总览

### ✅ 核心算法功能（优先级1 - 已完成）
1. **中文字符错误率（CER）计算**
   - 基于编辑距离算法（Levenshtein）
   - 支持中文字符级别的精确计算
   - 计算替换、删除、插入错误数

2. **中文文本预处理**
   - 基于jieba分词的中文文本预处理
   - 中文文本标准化（全/半角转换、数字统一等）
   - 精确的中文字符位置定位

3. **语气词过滤功能**
   - 过滤20种常见中文语气词（"嗯"、"啊"、"呢"等）
   - 基于jieba词性标注识别语气词(y)
   - 可选择启用/禁用语气词过滤

4. **详细错误分析**
   - 错误高亮显示功能
   - 详细的替换/删除/插入统计
   - 字符位置信息输出

### ✅ GUI界面功能（优先级1 - 已完成）
1. **分词器管理**
   - 智能分词器检测和选择
   - 实时状态显示（可用性、版本信息）
   - 分词器信息弹窗展示详细配置
   - 优雅降级机制

2. **文件管理**
   - 批量导入ASR文件和标注文件
   - 支持拖拽排序建立文件对应关系
   - 多种编码支持（UTF-8、GBK、GB2312、GB18030、ANSI）

3. **界面交互**
   - 800x650优化窗口，支持调整大小和最大化
   - 分层布局：分词器选择+文件管理+控制+结果展示
   - 左右对称的文件选择区域
   - 语气词过滤选项及悬停提示

4. **结果展示**
   - 表格形式显示统计结果
   - 显示文件名、字数、准确率、过滤状态、分词器类型
   - 支持导出为TXT或CSV格式

### ✅ 命令行工具（优先级2 - 已完成）
1. **基础功能**
   - 单文件对比计算
   - 详细错误分析选项（--details）
   - 语气词过滤选项（--filter-fillers）

2. **输出功能**
   - 详细评估报告
   - 错误高亮显示
   - 字符位置信息

## ✅ 三种分词工具切换功能架构设计（优先级1 - 已完成）

### 设计理念
采用 **"面向接口编程"** + **"策略模式"** 的设计思想，通过统一的分词器抽象层来解耦核心计算逻辑与具体分词工具实现。

### 1. 分词器抽象层设计

#### 1.1 创建模块化结构
```
dev/src/
├── text_tokenizers/
│   ├── __init__.py          # 基础抽象类和异常定义
│   └── tokenizers/          # 分词器实现
│       ├── base.py          # 基类和异常定义
│       ├── factory.py       # 分词器工厂和管理器
│       ├── jieba_tokenizer.py   # Jieba分词器实现
│       ├── thulac_tokenizer.py  # THULAC分词器实现
│       └── hanlp_tokenizer.py   # HanLP分词器实现
```

#### 1.2 核心接口定义
- **BaseTokenizer抽象基类**：定义通用接口
- **错误处理机制**：TokenizerError异常体系
- **初始化管理**：延迟加载和单例模式
- **文本验证**：输入检查和预处理

#### 1.3 具体分词器实现
- **JiebaTokenizer**：完全兼容现有功能
- **ThulacTokenizer**：处理tokenize方法的位置计算
- **HanlpTokenizer**：利用模型的偏移量输出
- **错误降级**：分词器不可用时的回退策略

### 2. 核心类改造

#### 2.1 ASRMetrics类重构
- **实例化模式**：从静态方法改为实例方法
- **分词器注入**：通过构造函数传入分词器
- **向后兼容**：保持API接口不变
- **错误恢复**：分词失败时的后备方案

#### 2.2 关键方法适配
- `preprocess_chinese_text()` - 使用可插拔分词器
- `filter_filler_words()` - 词性标注兼容性处理
- `get_character_positions()` - 位置信息获取统一化

### 3. 用户界面更新

#### 3.1 GUI界面增强
- **分词器选择控件**：下拉框选择可用分词器
- **状态指示**：显示分词器版本和可用性
- **动态检测**：运行时检查分词器状态
- **友好提示**：分词器不可用时的说明

#### 3.2 命令行工具扩展
- **--tokenizer参数**：指定使用的分词器
- **可用性检查**：列出当前可用的分词器
- **错误提示**：清晰的依赖缺失说明

### 4. 依赖管理策略

#### 4.1 分层依赖设计
```
核心依赖（必需）:
- jieba>=0.42.1
- jiwer>=2.5.0
- pandas>=1.3.5

可选依赖：
- thulac>=0.2.0
- hanlp>=2.1.0
- torch>=1.10.0  # HanLP需要
```

#### 4.2 安装和配置
- **install_optional_tokenizers.py**：可选分词器安装脚本
- **requirements-optional.txt**：可选依赖清单
- **动态检测**：运行时检查依赖可用性

### 5. 错误处理和兼容性

#### 5.1 多级错误处理
1. **依赖检查**：启动时检查可用分词器
2. **初始化失败**：提供清晰错误信息
3. **运行时异常**：自动降级到可用分词器
4. **用户友好**：非技术性错误提示

#### 5.2 性能优化策略
- **单例模式**：避免重复初始化分词器
- **懒加载**：按需加载分词器模型
- **缓存机制**：缓存分词结果（可选）
- **并发支持**：线程安全的分词器使用

### 6. 测试和验证

#### 6.1 测试覆盖
- **单元测试**：每个分词器的功能测试
- **集成测试**：ASRMetrics与不同分词器的集成
- **兼容性测试**：不同分词器结果的一致性验证
- **性能测试**：大文件处理的性能对比

#### 6.2 验证方法
- **功能验证**：确保所有分词器提供一致的接口
- **准确性验证**：对比不同分词器的CER计算结果
- **稳定性验证**：异常情况下的系统稳定性

## ✅ 开发完成总结

### ✅ 阶段一：基础架构搭建（已完成）
1. **创建分词器模块结构**
   - ✅ 设计抽象基类和异常体系
   - ✅ 实现Jieba分词器适配器
   - ✅ 创建工厂类和管理器

2. **重构ASRMetrics类**
   - ✅ 将静态方法改为实例方法
   - ✅ 集成分词器接口
   - ✅ 保持向后兼容性

3. **基础测试**
   - ✅ 编写单元测试
   - ✅ 验证基础功能正常

### ✅ 阶段二：分词器集成（已完成）
1. **实现THULAC分词器**
   - ✅ 处理依赖检查和初始化
   - ✅ 实现tokenize方法的位置计算
   - ✅ 错误处理和降级机制

2. **实现HanLP分词器**
   - ✅ 集成HanLP 2.x API
   - ✅ 处理模型下载和加载
   - ✅ 优化性能和内存使用

3. **完善工厂模式**
   - ✅ 动态分词器检测
   - ✅ 单例模式实现
   - ✅ 错误恢复机制

### ✅ 阶段三：界面和工具更新（已完成）
1. **GUI界面更新**
   - ✅ 添加分词器选择控件
   - ✅ 实现状态指示和提示
   - ✅ 集成到现有工作流程
   - ✅ 分词器信息弹窗功能

2. **命令行工具扩展**
   - ✅ 添加分词器参数
   - ✅ 更新帮助信息
   - ✅ 错误处理改进

3. **文档更新**
   - ✅ 更新README和使用说明
   - ✅ 添加分词器对比说明
   - ✅ 创建安装指南
   - ✅ UI定义文档完善

### ✅ 阶段四：测试和优化（已完成）
1. **全面测试**
   - ✅ 功能测试和集成测试
   - ✅ 性能测试和优化
   - ✅ 错误场景测试
   - ✅ 架构演示程序

2. **用户体验优化**
   - ✅ 错误提示优化
   - ✅ 性能调优（缓存机制）
   - ✅ 文档完善
   - ✅ 界面布局微调

3. **发布准备**
   - ✅ 版本管理
   - ✅ 发布说明
   - ✅ 部署测试

## 技术架构总结

### 架构优势
- **可扩展性**：基于接口的设计便于添加新分词器
- **稳定性**：多级错误处理确保系统稳定运行
- **兼容性**：向后兼容，不影响现有功能
- **性能**：单例和缓存机制优化性能

### 潜在风险及应对
1. **依赖复杂性**：通过可选依赖和动态检测降低风险
2. **性能差异**：提供性能对比和建议选择
3. **结果一致性**：通过标准化接口确保一致性
4. **维护成本**：模块化设计降低维护复杂度

## 总结

这个完善的架构设计通过引入分词器抽象层，实现了：
- **灵活性**：用户可根据需求选择最适合的分词器
- **可靠性**：完善的错误处理确保系统稳定
- **可扩展性**：未来可轻松添加更多分词器
- **用户友好**：清晰的界面和提示信息

该设计既保持了向后兼容性，又为项目带来了显著的功能增强，是一个平衡了技术先进性和实用性的优秀解决方案。




## 📊 **最新版Python程序代码统计**

### **总计代码行数：2,363行**

### 📁 **详细文件分解：**

| 文件 | 行数 | 占比 | 功能说明 |
|------|------|------|----------|
| **dev/src/main_with_tokenizers.py** | 665行 | 28.1% | 🎨 GUI主界面程序 |
| **dev/src/asr_metrics_refactored.py** | 520行 | 22.0% | 📊 重构后的计算引擎 |
| **dev/src/text_tokenizers/tokenizers/factory.py** | 301行 | 12.7% | 🏭 分词器工厂类 |
| **dev/src/text_tokenizers/tokenizers/hanlp_tokenizer.py** | 283行 | 12.0% | 🤖 HanLP分词器实现 |
| **dev/src/text_tokenizers/tokenizers/thulac_tokenizer.py** | 246行 | 10.4% | 🎓 THULAC分词器实现 |
| **dev/src/text_tokenizers/tokenizers/jieba_tokenizer.py** | 161行 | 6.8% | ⚡ Jieba分词器实现 |
| **dev/src/text_tokenizers/tokenizers/base.py** | 142行 | 6.0% | 🔧 抽象基类定义 |
| **dev/src/text_tokenizers/__init__.py** | 45行 | 1.9% | 📦 模块导出接口 |

### 🎯 **架构特点分析：**

1. **GUI界面占主导** (28.1%)：用户交互是核心
2. **计算引擎扎实** (22.0%)：核心算法实现完善  
3. **分词器架构完整** (49.8%)：多分词器支持架构完善
4. **代码结构清晰**：每个模块职责明确，没有冗余

### 📈 **版本对比：**
- **最新版本**：2,363行（多分词器架构）
- **旧版本** (v0.1.0)：801行（单一jieba分词器）
- **代码增长**：+195% 

**新增的1,562行代码主要用于：**
- 🧠 多分词器抽象架构 (933行)
- 🎨 GUI界面功能增强 (237行) 
- 📊 计算引擎重构优化 (392行)

总的来说，**2,363行**的代码量体现了一个功能完善、架构清晰的专业级ASR字准确率分析工具！🚀

## 📁 **最新项目目录结构调整（2024年12月）**

### ✅ 目录结构重组完成
为了符合项目cursorrules规范，已完成以下目录结构调整：

#### **新的规范化目录结构：**
```
cer-matchingtools/
├── dev/                             # 开发目录（新增）
│   ├── src/                         # 所有源代码（从根目录/src移入）
│   │   ├── main_with_tokenizers.py          # 主程序入口
│   │   ├── asr_metrics_refactored.py        # CER计算引擎
│   │   ├── requirements.txt                 # 依赖声明
│   │   ├── text_tokenizers/                 # 分词器模块
│   │   │   ├── __init__.py                  # 模块初始化
│   │   │   └── tokenizers/                  # 分词器实现
│   │   │       ├── base.py                  # 基类和异常定义
│   │   │       ├── factory.py               # 工厂类和管理器
│   │   │       ├── jieba_tokenizer.py       # Jieba分词器
│   │   │       ├── thulac_tokenizer.py      # THULAC分词器
│   │   │       └── hanlp_tokenizer.py       # HanLP分词器
│   │   └── v0.1.0/                         # 历史版本代码
│   └── output/                      # 开发过程输出文件（新增）
├── docs/                           # 项目文档目录
│   ├── Cer-MatchingTools-V1-架构设计.md          # 新增架构设计文档
│   ├── Cer-MatchingTools-V1-需求规格说明书.md     # 新增需求规格说明书
│   ├── Cer-MatchingTools-V1-项目管理.md          # 项目管理文档
│   ├── Cer-MatchingTools-V1-UIdefinition.md     # UI设计说明文档
│   └── 其他设计文档...
├── tests/                          # 测试脚本和文档目录
├── release/                        # 发布文件目录
└── ref/                            # 参考资料目录（新增）
    ├── demo/                       # 示例文件（从根目录移入）
    │   ├── asr_result2.txt
    │   ├── ref_text2.txt
    │   └── ...
    └── logo/                       # 项目logo（从根目录移入）
        └── cer-logo.png
```

### ✅ 调整内容总结：

#### **已完成的结构调整：**
1. **✅ 创建/dev目录**：符合cursorrules开发目录要求
2. **✅ 移动源代码**：将/src目录内容移动到/dev/src/
3. **✅ 创建/dev/output目录**：用于开发过程输出文件
4. **✅ 创建/ref目录**：存放参考资料和文档
5. **✅ 移动demo文件**：将示例文件移入/ref/demo/
6. **✅ 移动logo文件**：将logo移入/ref/logo/
7. **✅ 创建架构设计文档**：完善重要文档
8. **✅ 创建需求规格说明书**：完善重要文档

#### **符合cursorrules的改进：**
- ✅ 开发代码统一放在/dev/src目录
- ✅ 输出文件统一放在/dev/output目录（预留）
- ✅ 测试相关放在/tests目录
- ✅ 发布文件放在/release目录
- ✅ 参考资料放在/ref目录且不允许修改
- ✅ 重要文档使用规范前缀命名

#### **文档完善情况：**
- ✅ 架构设计文档：`Cer-MatchingTools-V1-架构设计.md`
- ✅ 需求规格说明书：`Cer-MatchingTools-V1-需求规格说明书.md`
- ✅ 项目管理文档：`Cer-MatchingTools-V1-项目管理.md`
- ✅ UI设计说明文档：`Cer-MatchingTools-V1-UIdefinition.md`

### **规范化效果：**
本次目录结构调整完全符合项目cursorrules要求，为后续开发提供了清晰的目录规范和完善的文档体系。