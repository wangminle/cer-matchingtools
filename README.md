# ASR Character Accuracy Comparison Tool

A Python-based tool for batch comparing character accuracy rates between ASR (Automatic Speech Recognition) transcription results and standard text, with multi-tokenizer support.

## âœ¨ Core Features

### ğŸ¯ Multi-Tokenizer Support
- **Jieba Tokenizer**: Default choice, high-speed segmentation, suitable for daily use
- **THULAC Tokenizer**: Developed by Tsinghua University, high-precision segmentation, suitable for professional analysis
- **HanLP Tokenizer**: Deep learning model, highest precision, suitable for research environments

### ğŸš€ Smart Features
- âœ… **Automatic Tokenizer Detection**: Detects installed tokenizers at startup
- âœ… **Smart Fallback Mechanism**: Automatically fallback to jieba when tokenizers are unavailable
- âœ… **Real-time Status Display**: GUI shows tokenizer status and version information
- âœ… **Dependency-free Demo**: Complete architecture demonstration without additional dependencies

### ğŸ“Š Advanced Functions
- Batch import ASR transcription result documents and standard annotation documents
- Drag-and-drop to establish one-to-one correspondence between ASR results and annotation files
- Automatically calculate Character Accuracy Rate
- Count document character information
- Support exporting statistical results in TXT or CSV format
- Support multiple text encodings (UTF-8, GBK, GB2312, GB18030, ANSI)
- **Filler word filtering**: Optional filtering of filler words like "å—¯", "å•Š"
- **Optimized user interface**: Larger result display area, more user-friendly experience

## ğŸ“¦ Installation & Dependencies

### Quick Experience (Recommended)
```bash
# Experience complete architecture without any dependencies
cd cer-matchingtools
python3 tests/test_architecture_demo.py
```

### Full Installation
```bash
# Install core dependencies
pip install -r requirements.txt

# Optional: Install other tokenizers
pip install thulac    # Install THULAC tokenizer
pip install hanlp     # Install HanLP tokenizer (large, first use requires model download)
```

#### Dependency Description
**Core Dependencies (Required):**
- `jieba>=0.42.1`: Default Chinese tokenizer
- `jiwer>=2.5.0`: Text preprocessing and error rate calculation
- `pandas>=1.3.0`: Data processing and export
- `python-Levenshtein>=0.12.2`: Efficient edit distance calculation

**Optional Dependencies:**
- `thulac>=0.2.0`: THULAC high-precision tokenizer
- `hanlp>=2.1.0`: HanLP deep learning tokenizer

## ğŸ® Usage

### 1. GUI Mode (Recommended)

```bash
python3 src/main_with_tokenizers.py
```

#### Operation Steps:
1. **Select Tokenizer**: Choose the desired tokenizer in the top dropdown
2. **Check Status**: Confirm tokenizer status shows green âœ“
3. **Import Files**:
   - Left: Click "Select ASR Files" to batch import ASR transcription results
   - Right: Click "Select Annotation Files" to batch import standard annotation files
4. **Establish Correspondence**: Adjust file order by drag-and-drop
5. **Configure Options**: Check "Filter Filler Words" as needed
6. **Calculate Statistics**: Click "Start Calculation" button
7. **View Results**: Result table shows detailed statistics
8. **Export Data**: Click "Export Results" to save as file

#### Interface Function Description:
- **Tokenizer Selection Area**: Select and manage tokenizers
- **File Selection Area**: Import and manage file lists
- **Control Area**: Statistics button and option configuration
- **Result Display Area**: Detailed statistical result table

### 2. Architecture Demo Mode

```bash
python3 tests/test_architecture_demo.py
```

This demo program:
- ğŸ“š **No external dependencies required**
- ğŸ”¬ **Shows complete architecture design**
- âš¡ **Quick function verification**
- ğŸ§ª **Suitable for development testing**

### 3. Command-Line Tool Mode

Compare two specified files:

```bash
# Basic usage
python3 src/check_accuracy.py --ref reference_file.txt --asr asr_result.txt

# Show detailed error analysis
python3 src/check_accuracy.py --ref reference_file.txt --asr asr_result.txt --details

# Filter filler words
python3 src/check_accuracy.py --ref reference_file.txt --asr asr_result.txt --filter-fillers
```

## ğŸ¯ Tokenizer Selection Guide

### Jieba Tokenizer
- **Performance**: âš¡ High Speed
- **Accuracy**: â­â­â­ Medium
- **Use Cases**: Daily batch processing, quick verification
- **Advantages**: Fast speed, low resource usage, good compatibility

### THULAC Tokenizer
- **Performance**: âš¡âš¡ Medium Speed
- **Accuracy**: â­â­â­â­ High Precision
- **Use Cases**: Professional analysis, high quality requirements
- **Advantages**: Developed by Tsinghua University, academic standards, accurate POS tagging

### HanLP Tokenizer
- **Performance**: âš¡ Slower (first use requires model download)
- **Accuracy**: â­â­â­â­â­ Highest Precision
- **Use Cases**: Research environments, highest precision requirements
- **Advantages**: Deep learning models, multi-task support, continuous updates

## ğŸ“ Character Accuracy Calculation Method

Uses the complement of Character Error Rate (CER):

```
Character Accuracy = 1 - CER = 1 - (S + D + I) / N
```

Where:
- **S**: Number of substitution errors
- **D**: Number of deletion errors
- **I**: Number of insertion errors
- **N**: Total number of characters in the standard text

### ğŸ”§ Improved Calculation Process

1. **Tokenization Preprocessing**: Use selected tokenizer for text segmentation
2. **Text Normalization**: Process full/half-width characters, unify numerical expressions
3. **Filler Word Filtering (Optional)**: Filter filler words like "å—¯", "å•Š", "å‘¢"
4. **Character Position Localization**: Precisely locate each character's position in original text
5. **Edit Distance Calculation**: Use Levenshtein distance algorithm
6. **Error Analysis**: Identify substitution, deletion, insertion errors with visualization

## ğŸ“ Project Structure

```
cer-matchingtools/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ text_tokenizers/           # ğŸ§  Core tokenizer module
â”‚   â”‚   â”œâ”€â”€ __init__.py            # Module export interface
â”‚   â”‚   â””â”€â”€ tokenizers/            # Tokenizer implementations
â”‚   â”‚       â”œâ”€â”€ base.py            # Abstract base class
â”‚   â”‚       â”œâ”€â”€ factory.py         # Factory class
â”‚   â”‚       â”œâ”€â”€ jieba_tokenizer.py # Jieba implementation
â”‚   â”‚       â”œâ”€â”€ thulac_tokenizer.py# THULAC implementation
â”‚   â”‚       â””â”€â”€ hanlp_tokenizer.py # HanLP implementation
â”‚   â”œâ”€â”€ main_with_tokenizers.py    # ğŸ¨ New GUI interface
â”‚   â”œâ”€â”€ asr_metrics_refactored.py  # ğŸ“Š Refactored calculation engine
â”‚   â””â”€â”€ v0.1.0/                    # ğŸ“¦ Original version backup
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_architecture_demo.py  # ğŸ”¬ Architecture demo program
â”œâ”€â”€ docs/                          # ğŸ“š Detailed documentation
â”œâ”€â”€ demo/                          # ğŸ§ª Example files
â”œâ”€â”€ requirements.txt               # ğŸ“¦ Dependency management
â””â”€â”€ QUICK_START.md                 # ğŸš€ Quick start guide
```

## ğŸ”§ Troubleshooting

### Common Issues

**Q: How to handle unavailable tokenizers?**
A: Check if corresponding dependencies are installed:
```bash
pip install thulac    # Install THULAC
pip install hanlp     # Install HanLP
```

**Q: Why is HanLP slow on first use?**
A: HanLP needs to download deep learning models, first use requires patience. Recommend using in good network environment.

**Q: How to verify architecture design?**
A: Run the architecture demo program:
```bash
python3 tests/test_architecture_demo.py
```

**Q: How to choose the right tokenizer?**
A: Refer to tokenizer selection guide, choose based on speed and accuracy needs:
- For speed: Choose Jieba
- For balance: Choose THULAC
- For precision: Choose HanLP

## ğŸ†• Version Features

### Current Version Highlights
- ğŸ¯ **Multi-tokenizer Architecture**: Support for three mainstream Chinese tokenizers
- ğŸš€ **Smart Switching**: Automatic detection and graceful fallback
- ğŸ¨ **Optimized Interface**: More user-friendly experience
- ğŸ”¬ **Architecture Demo**: Complete feature demonstration without dependencies
- ğŸ“Š **Detailed Statistics**: Enhanced result display and analysis

### Backward Compatibility
- âœ… Maintain original API interfaces unchanged
- âœ… Default to jieba tokenizer
- âœ… Support original file formats and encodings

## ğŸ“ Technical Support

For issues, please check:
- `QUICK_START.md` - Quick start guide
- `docs/project_development_summary.md` - Detailed development summary
- `docs/test_cases.md` - Test case documentation
- `tests/test_architecture_demo.py` - Architecture demo code

## ğŸ“„ License

This project is released under an open source license, see `LICENSE` file for details.

---

ğŸ‰ **Experience multi-tokenizer switching now to improve ASR character accuracy analysis precision and efficiency!** 