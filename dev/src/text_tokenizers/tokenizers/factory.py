"""
åˆ†è¯å™¨å·¥å‚
æä¾›åˆ†è¯å™¨çš„åˆ›å»ºã€ç®¡ç†å’Œè·å–åŠŸèƒ½
"""

from typing import Dict, List, Optional, Any
from .base import BaseTokenizer, TokenizerInitError
from .jieba_tokenizer import JiebaTokenizer
from .thulac_tokenizer import ThulacTokenizer
from .hanlp_tokenizer import HanlpTokenizer


class TokenizerFactory:
    """
    åˆ†è¯å™¨å·¥å‚ç±»
    ä½¿ç”¨å•ä¾‹æ¨¡å¼ç®¡ç†åˆ†è¯å™¨å®ä¾‹ï¼Œé¿å…é‡å¤åˆå§‹åŒ–
    """
    
    _instance = None
    _tokenizers: Dict[str, BaseTokenizer] = {}
    _available_tokenizers: Dict[str, type] = {
        'jieba': JiebaTokenizer,
        'thulac': ThulacTokenizer,
        'hanlp': HanlpTokenizer
    }
    
    def __new__(cls):
        """
        å•ä¾‹æ¨¡å¼å®ç°
        ç¡®ä¿å·¥å‚ç±»åœ¨æ•´ä¸ªåº”ç”¨ç¨‹åºä¸­åªæœ‰ä¸€ä¸ªå®ä¾‹
        
        Returns:
            TokenizerFactory: å·¥å‚ç±»çš„å”¯ä¸€å®ä¾‹
        """
        if cls._instance is None:
            cls._instance = super(TokenizerFactory, cls).__new__(cls)
        return cls._instance
    
    @classmethod
    def get_available_tokenizers(cls) -> List[str]:
        """
        è·å–å¯ç”¨çš„åˆ†è¯å™¨åˆ—è¡¨
        æ£€æŸ¥æ¯ä¸ªåˆ†è¯å™¨çš„ä¾èµ–æ˜¯å¦æ»¡è¶³
        
        Returns:
            List[str]: å¯ç”¨çš„åˆ†è¯å™¨åç§°åˆ—è¡¨
        """
        available = []
        
        for name, tokenizer_class in cls._available_tokenizers.items():
            try:
                # å°è¯•åˆ›å»ºåˆ†è¯å™¨å®ä¾‹
                tokenizer = tokenizer_class()
                tokenizer.initialize()
                available.append(name)
            except TokenizerInitError:
                # å¦‚æœåˆå§‹åŒ–å¤±è´¥ï¼Œè¯¥åˆ†è¯å™¨ä¸å¯ç”¨
                continue
            except Exception:
                # å…¶ä»–å¼‚å¸¸ä¹Ÿè§†ä¸ºä¸å¯ç”¨
                continue
        
        # ç¡®ä¿jiebaæ€»æ˜¯åœ¨åˆ—è¡¨ä¸­ï¼ˆå¦‚æœå¯ç”¨çš„è¯ï¼‰
        if 'jieba' not in available:
            # å†æ¬¡å°è¯•jieba
            try:
                jieba_tokenizer = JiebaTokenizer()
                jieba_tokenizer.initialize()
                available.insert(0, 'jieba')
            except:
                pass
        
        return available
    
    @classmethod
    def get_tokenizer(cls, name: str) -> BaseTokenizer:
        """
        è·å–åˆ†è¯å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
        
        Args:
            name (str): åˆ†è¯å™¨åç§°
            
        Returns:
            BaseTokenizer: åˆ†è¯å™¨å®ä¾‹
            
        Raises:
            ValueError: å¦‚æœåˆ†è¯å™¨åç§°ä¸æ”¯æŒ
            TokenizerInitError: å¦‚æœåˆ†è¯å™¨åˆå§‹åŒ–å¤±è´¥
        """
        if name not in cls._available_tokenizers:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†è¯å™¨: {name}ï¼Œå¯ç”¨çš„åˆ†è¯å™¨: {list(cls._available_tokenizers.keys())}")
        
        # å¦‚æœå·²ç»åˆ›å»ºè¿‡è¯¥åˆ†è¯å™¨å®ä¾‹ï¼Œç›´æ¥è¿”å›
        if name in cls._tokenizers:
            return cls._tokenizers[name]
        
        # åˆ›å»ºæ–°çš„åˆ†è¯å™¨å®ä¾‹
        tokenizer_class = cls._available_tokenizers[name]
        tokenizer = tokenizer_class()
        
        # åˆå§‹åŒ–åˆ†è¯å™¨
        try:
            success = tokenizer.initialize()
            if not success:
                raise TokenizerInitError(f"{name}åˆ†è¯å™¨åˆå§‹åŒ–å¤±è´¥")
        except Exception as e:
            raise TokenizerInitError(f"{name}åˆ†è¯å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        
        # ç¼“å­˜åˆ†è¯å™¨å®ä¾‹
        cls._tokenizers[name] = tokenizer
        
        return tokenizer
    
    @classmethod
    def get_tokenizer_info(cls, name: str) -> Dict[str, Any]:
        """
        è·å–åˆ†è¯å™¨ä¿¡æ¯
        
        Args:
            name (str): åˆ†è¯å™¨åç§°
            
        Returns:
            Dict[str, Any]: åˆ†è¯å™¨ä¿¡æ¯å­—å…¸
        """
        if name not in cls._available_tokenizers:
            return {
                'name': name,
                'available': False,
                'error': f'ä¸æ”¯æŒçš„åˆ†è¯å™¨: {name}'
            }
        
        # ğŸ”§ ä¿®å¤: ä¼˜å…ˆä»ç¼“å­˜è·å–ä¿¡æ¯ï¼Œé¿å…é‡å¤åˆå§‹åŒ–
        if name in cls._tokenizers:
            try:
                cached_tokenizer = cls._tokenizers[name]
                info = cached_tokenizer.get_info()
                info['available'] = True
                # ç¡®ä¿ç¼“å­˜çš„å®ä¾‹æ˜¾ç¤ºæ­£ç¡®çš„åˆå§‹åŒ–çŠ¶æ€
                info['initialized'] = cached_tokenizer.is_initialized
                return info
            except Exception as e:
                print(f"ä»ç¼“å­˜è·å–{name}åˆ†è¯å™¨ä¿¡æ¯å¤±è´¥: {str(e)}")
        
        # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼Œå†å°è¯•åˆ›å»ºæ–°å®ä¾‹
        try:
            tokenizer = cls.get_tokenizer(name)
            info = tokenizer.get_info()
            info['available'] = True
            return info
        except Exception as e:
            return {
                'name': name,
                'available': False,
                'error': str(e)
            }
    
    @classmethod
    def get_cached_tokenizer_info(cls, name: str) -> Dict[str, Any]:
        """
        è·å–å·²ç¼“å­˜åˆ†è¯å™¨çš„ä¿¡æ¯ï¼ˆä¸ä¼šè§¦å‘æ–°çš„åˆå§‹åŒ–ï¼‰
        
        Args:
            name (str): åˆ†è¯å™¨åç§°
            
        Returns:
            Dict[str, Any]: åˆ†è¯å™¨ä¿¡æ¯å­—å…¸ï¼Œå¦‚æœæœªç¼“å­˜è¿”å›None
        """
        if name not in cls._available_tokenizers:
            return None
        
        if name in cls._tokenizers:
            try:
                cached_tokenizer = cls._tokenizers[name]
                info = cached_tokenizer.get_info()
                info['available'] = True
                info['initialized'] = cached_tokenizer.is_initialized
                info['cached'] = True
                return info
            except Exception as e:
                print(f"ä»ç¼“å­˜è·å–{name}åˆ†è¯å™¨ä¿¡æ¯å¤±è´¥: {str(e)}")
                return None
        
        return None
    
    @classmethod
    def check_tokenizer_availability(cls, name: str) -> bool:
        """
        æ£€æŸ¥æŒ‡å®šåˆ†è¯å™¨æ˜¯å¦å¯ç”¨
        
        Args:
            name (str): åˆ†è¯å™¨åç§°
            
        Returns:
            bool: æ˜¯å¦å¯ç”¨
        """
        if name not in cls._available_tokenizers:
            return False
        
        try:
            tokenizer_class = cls._available_tokenizers[name]
            tokenizer = tokenizer_class()
            return tokenizer.initialize()
        except:
            return False
    
    @classmethod
    def create_tokenizer(cls, name: str) -> BaseTokenizer:
        """
        åˆ›å»ºæ–°çš„åˆ†è¯å™¨å®ä¾‹ï¼ˆä¸ä½¿ç”¨ç¼“å­˜ï¼‰
        
        Args:
            name (str): åˆ†è¯å™¨åç§°
            
        Returns:
            BaseTokenizer: æ–°çš„åˆ†è¯å™¨å®ä¾‹
            
        Raises:
            ValueError: å¦‚æœåˆ†è¯å™¨åç§°ä¸æ”¯æŒ
            TokenizerInitError: å¦‚æœåˆ†è¯å™¨åˆå§‹åŒ–å¤±è´¥
        """
        if name not in cls._available_tokenizers:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†è¯å™¨: {name}")
        
        tokenizer_class = cls._available_tokenizers[name]
        tokenizer = tokenizer_class()
        
        # åˆå§‹åŒ–åˆ†è¯å™¨
        try:
            success = tokenizer.initialize()
            if not success:
                raise TokenizerInitError(f"{name}åˆ†è¯å™¨åˆå§‹åŒ–å¤±è´¥")
        except Exception as e:
            raise TokenizerInitError(f"{name}åˆ†è¯å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        
        return tokenizer
    
    @classmethod
    def clear_cache(cls):
        """
        æ¸…é™¤ç¼“å­˜çš„åˆ†è¯å™¨å®ä¾‹
        """
        cls._tokenizers.clear()
    
    @classmethod
    def get_all_tokenizer_info(cls) -> Dict[str, Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰åˆ†è¯å™¨çš„ä¿¡æ¯
        
        Returns:
            Dict[str, Dict[str, Any]]: æ‰€æœ‰åˆ†è¯å™¨çš„ä¿¡æ¯å­—å…¸
        """
        info = {}
        for name in cls._available_tokenizers.keys():
            info[name] = cls.get_tokenizer_info(name)
        return info


# ä¾¿æ·å‡½æ•°
def get_tokenizer(name: str) -> BaseTokenizer:
    """
    è·å–åˆ†è¯å™¨å®ä¾‹çš„ä¾¿æ·å‡½æ•°
    
    Args:
        name (str): åˆ†è¯å™¨åç§°
        
    Returns:
        BaseTokenizer: åˆ†è¯å™¨å®ä¾‹
    """
    factory = TokenizerFactory()
    return factory.get_tokenizer(name)


def get_available_tokenizers() -> List[str]:
    """
    è·å–å¯ç”¨åˆ†è¯å™¨åˆ—è¡¨çš„ä¾¿æ·å‡½æ•°
    
    Returns:
        List[str]: å¯ç”¨çš„åˆ†è¯å™¨åç§°åˆ—è¡¨
    """
    factory = TokenizerFactory()
    return factory.get_available_tokenizers()


def get_tokenizer_info(name: str) -> Dict[str, Any]:
    """
    è·å–åˆ†è¯å™¨ä¿¡æ¯çš„ä¾¿æ·å‡½æ•°
    
    Args:
        name (str): åˆ†è¯å™¨åç§°
        
    Returns:
        Dict[str, Any]: åˆ†è¯å™¨ä¿¡æ¯å­—å…¸
    """
    factory = TokenizerFactory()
    return factory.get_tokenizer_info(name)


def get_cached_tokenizer_info(name: str) -> Dict[str, Any]:
    """
    è·å–å·²ç¼“å­˜åˆ†è¯å™¨ä¿¡æ¯çš„ä¾¿æ·å‡½æ•°
    
    Args:
        name (str): åˆ†è¯å™¨åç§°
        
    Returns:
        Dict[str, Any]: åˆ†è¯å™¨ä¿¡æ¯å­—å…¸ï¼Œå¦‚æœæœªç¼“å­˜è¿”å›None
    """
    factory = TokenizerFactory()
    return factory.get_cached_tokenizer_info(name)