"""
HanLPåˆ†è¯å™¨å®ç°
åŸºäºHanLPåº“çš„åˆ†è¯å™¨ï¼Œæä¾›BERTç­‰æ·±åº¦å­¦ä¹ æ¨¡å‹æ”¯æŒ
"""

from typing import List, Tuple
from .base import BaseTokenizer, TokenizerInitError, TokenizerProcessError


class HanlpTokenizer(BaseTokenizer):
    """
    HanLPåˆ†è¯å™¨å®ç°
    åŸºäºHanLP 2.xç‰ˆæœ¬ï¼Œæ”¯æŒBERTç­‰æ·±åº¦å­¦ä¹ æ¨¡å‹çš„ä¸­æ–‡åˆ†è¯
    """
    
    def __init__(self):
        super().__init__()
        self.name = "hanlp"
        self.hanlp = None
        self.tok_model = None
        self.pos_model = None
    
    def initialize(self) -> bool:
        """
        åˆå§‹åŒ–HanLPåˆ†è¯å™¨
        
        Returns:
            bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
            
        Raises:
            TokenizerInitError: åˆå§‹åŒ–å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            # æ£€æŸ¥HanLPæ˜¯å¦å¯ç”¨
            try:
                import hanlp
                self.hanlp = hanlp
            except ImportError:
                raise TokenizerInitError("HanLPåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install hanlp")
            
            # åˆå§‹åŒ–åˆ†è¯æ¨¡å‹
            try:
                # ä½¿ç”¨HanLPçš„ä¸­æ–‡åˆ†è¯æ¨¡å‹
                # ä¼˜å…ˆä½¿ç”¨BERTæ¨¡å‹ï¼Œå¦‚æœä¸å¯ç”¨åˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
                try:
                    self.tok_model = hanlp.load(hanlp.pretrained.tok.SIGHAN2005_PKU_BERT_BASE_ZH)
                except:
                    try:
                        self.tok_model = hanlp.load(hanlp.pretrained.tok.CTB6_CONVSEG)
                    except:
                        # å¦‚æœé¢„è®­ç»ƒæ¨¡å‹ä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨æœ€åŸºç¡€çš„æ¨¡å‹
                        self.tok_model = hanlp.load('PKU_NAME_MERGED_SYS_OPEN')
                
                # åˆå§‹åŒ–è¯æ€§æ ‡æ³¨æ¨¡å‹
                try:
                    self.pos_model = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH)
                except:
                    # å¦‚æœè¯æ€§æ ‡æ³¨æ¨¡å‹ä¸å¯ç”¨ï¼Œè®¾ä¸ºNoneï¼Œåç»­ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ
                    self.pos_model = None
                
                # æµ‹è¯•åˆ†è¯å™¨æ˜¯å¦æ­£å¸¸å·¥ä½œ
                test_result = self.tok_model("æµ‹è¯•")
                if not test_result:
                    raise TokenizerInitError("HanLPåˆ†è¯å™¨æµ‹è¯•å¤±è´¥")
                
                # è·å–ç‰ˆæœ¬ä¿¡æ¯
                try:
                    # ğŸ”§ ä¼˜åŒ–: ä½¿ç”¨å¤šç§æ–¹å¼è·å–ç‰ˆæœ¬ä¿¡æ¯
                    try:
                        from importlib.metadata import version
                        self.version = version('hanlp')
                    except ImportError:
                        # Python < 3.8 ä½¿ç”¨importlib_metadata
                        try:
                            from importlib_metadata import version
                            self.version = version('hanlp')
                        except ImportError:
                            # å¤‡ç”¨æ–¹æ¡ˆï¼šä»æ¨¡å—å±æ€§è·å–
                            self.version = getattr(hanlp, '__version__', 'unknown')
                    except Exception as e:
                        print(f"è·å–HanLPç‰ˆæœ¬å¤±è´¥: {str(e)}")
                        self.version = getattr(hanlp, '__version__', 'unknown')
                except Exception:
                    self.version = 'unknown'
                
                self.is_initialized = True
                return True
                
            except Exception as e:
                raise TokenizerInitError(f"HanLPæ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
                
        except Exception as e:
            raise TokenizerInitError(f"HanLPåˆ†è¯å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
    def cut(self, text: str) -> List[str]:
        """
        åŸºç¡€åˆ†è¯åŠŸèƒ½
        
        Args:
            text (str): å¾…åˆ†è¯çš„æ–‡æœ¬
            
        Returns:
            List[str]: åˆ†è¯ç»“æœåˆ—è¡¨
            
        Raises:
            TokenizerProcessError: åˆ†è¯å¤„ç†å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            if not self.is_initialized:
                raise TokenizerProcessError("HanLPåˆ†è¯å™¨æœªåˆå§‹åŒ–")
            
            cleaned_text = self.validate_text(text)
            if not cleaned_text:
                return []
            
            # ä½¿ç”¨HanLPè¿›è¡Œåˆ†è¯
            result = self.tok_model(cleaned_text)
            return result
            
        except Exception as e:
            raise TokenizerProcessError(f"HanLPåˆ†è¯å¤±è´¥: {str(e)}")
    
    def posseg(self, text: str) -> List[Tuple[str, str]]:
        """
        è¯æ€§æ ‡æ³¨åŠŸèƒ½
        
        Args:
            text (str): å¾…æ ‡æ³¨çš„æ–‡æœ¬
            
        Returns:
            List[Tuple[str, str]]: (è¯è¯­, è¯æ€§)çš„å…ƒç»„åˆ—è¡¨
            
        Raises:
            TokenizerProcessError: è¯æ€§æ ‡æ³¨å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            if not self.is_initialized:
                raise TokenizerProcessError("HanLPåˆ†è¯å™¨æœªåˆå§‹åŒ–")
            
            cleaned_text = self.validate_text(text)
            if not cleaned_text:
                return []
            
            # å…ˆè¿›è¡Œåˆ†è¯
            words = self.cut(cleaned_text)
            
            if self.pos_model:
                # å¦‚æœæœ‰è¯æ€§æ ‡æ³¨æ¨¡å‹ï¼Œä½¿ç”¨HanLPè¿›è¡Œè¯æ€§æ ‡æ³¨
                try:
                    pos_tags = self.pos_model(words)
                    result = list(zip(words, pos_tags))
                    return result
                except Exception as e:
                    # å¦‚æœè¯æ€§æ ‡æ³¨å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è¯æ€§
                    result = [(word, 'unk') for word in words]
                    return result
            else:
                # å¦‚æœæ²¡æœ‰è¯æ€§æ ‡æ³¨æ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤è¯æ€§
                result = [(word, 'unk') for word in words]
                return result
            
        except Exception as e:
            raise TokenizerProcessError(f"HanLPè¯æ€§æ ‡æ³¨å¤±è´¥: {str(e)}")
    
    def tokenize(self, text: str) -> List[Tuple[str, int, int]]:
        """
        ç²¾ç¡®åˆ†è¯ï¼Œè¿”å›è¯è¯­åŠå…¶åœ¨åŸæ–‡ä¸­çš„ä½ç½®
        HanLPæŸäº›æ¨¡å‹æ”¯æŒè¾“å‡ºåç§»é‡ï¼Œå¦‚æœä¸æ”¯æŒåˆ™æ‰‹åŠ¨è®¡ç®—
        
        Args:
            text (str): å¾…åˆ†è¯çš„æ–‡æœ¬
            
        Returns:
            List[Tuple[str, int, int]]: (è¯è¯­, å¼€å§‹ä½ç½®, ç»“æŸä½ç½®)çš„å…ƒç»„åˆ—è¡¨
            
        Raises:
            TokenizerProcessError: åˆ†è¯å¤„ç†å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            if not self.is_initialized:
                raise TokenizerProcessError("HanLPåˆ†è¯å™¨æœªåˆå§‹åŒ–")
            
            cleaned_text = self.validate_text(text)
            if not cleaned_text:
                return []
            
            # å°è¯•ä½¿ç”¨HanLPçš„åç§»é‡åŠŸèƒ½
            try:
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒè¾“å‡ºåç§»é‡
                if hasattr(self.tok_model, 'config') and hasattr(self.tok_model.config, 'output_offsets'):
                    # å¯ç”¨åç§»é‡è¾“å‡º
                    self.tok_model.config.output_offsets = True
                    result_with_offsets = self.tok_model(cleaned_text)
                    
                    if isinstance(result_with_offsets, tuple) and len(result_with_offsets) == 2:
                        tokens, offsets = result_with_offsets
                        result = []
                        for token, (start, end) in zip(tokens, offsets):
                            result.append((token, start, end))
                        return result
            except:
                pass  # å¦‚æœä¸æ”¯æŒåç§»é‡ï¼Œç»§ç»­ä½¿ç”¨æ‰‹åŠ¨è®¡ç®—
            
            # æ‰‹åŠ¨è®¡ç®—ä½ç½®ä¿¡æ¯
            words = self.cut(cleaned_text)
            result = []
            current_pos = 0
            text_chars = list(cleaned_text)
            
            for word in words:
                word_chars = list(word)
                word_len = len(word_chars)
                
                # åœ¨å‰©ä½™æ–‡æœ¬ä¸­æŸ¥æ‰¾å½“å‰è¯è¯­
                start_pos = current_pos
                found = False
                
                # å‘å‰æœç´¢åŒ¹é…ä½ç½®
                while start_pos <= len(text_chars) - word_len:
                    # æ£€æŸ¥æ˜¯å¦åŒ¹é…
                    match = True
                    for i, char in enumerate(word_chars):
                        if start_pos + i >= len(text_chars) or text_chars[start_pos + i] != char:
                            match = False
                            break
                    
                    if match:
                        end_pos = start_pos + word_len
                        result.append((word, start_pos, end_pos))
                        current_pos = end_pos
                        found = True
                        break
                    
                    start_pos += 1
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…ï¼Œä½¿ç”¨è¿‘ä¼¼ä½ç½®
                if not found:
                    end_pos = current_pos + word_len
                    if end_pos > len(text_chars):
                        end_pos = len(text_chars)
                    result.append((word, current_pos, end_pos))
                    current_pos = end_pos
            
            return result
            
        except Exception as e:
            raise TokenizerProcessError(f"HanLPç²¾ç¡®åˆ†è¯å¤±è´¥: {str(e)}")
    
    def get_info(self) -> dict:
        """
        è·å–HanLPåˆ†è¯å™¨ä¿¡æ¯
        
        Returns:
            dict: åŒ…å«åˆ†è¯å™¨è¯¦ç»†ä¿¡æ¯çš„å­—å…¸
        """
        info = super().get_info()
        
        # è·å–æ¨¡å‹ä¿¡æ¯
        tok_model_name = 'unknown'
        pos_model_name = 'unknown'
        
        if self.tok_model:
            if hasattr(self.tok_model, 'model_name'):
                tok_model_name = self.tok_model.model_name
            elif hasattr(self.tok_model, '__class__'):
                tok_model_name = self.tok_model.__class__.__name__
        
        if self.pos_model:
            if hasattr(self.pos_model, 'model_name'):
                pos_model_name = self.pos_model.model_name
            elif hasattr(self.pos_model, '__class__'):
                pos_model_name = self.pos_model.__class__.__name__
        
        info.update({
            'description': 'åŸºäºHanLPåº“çš„æ·±åº¦å­¦ä¹ ä¸­æ–‡åˆ†è¯å™¨',
            'features': ['åˆ†è¯', 'è¯æ€§æ ‡æ³¨', 'ç²¾ç¡®ä½ç½®åˆ†è¯', 'BERTæ”¯æŒ'],
            'dependencies': ['hanlp'],
            'performance': 'è¾ƒæ…¢ï¼ˆæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼‰',
            'accuracy': 'æœ€é«˜ç²¾åº¦',
            'tok_model': tok_model_name,
            'pos_model': pos_model_name,
            'note': 'é¦–æ¬¡ä½¿ç”¨æ—¶éœ€è¦ä¸‹è½½æ¨¡å‹æ–‡ä»¶'
        })
        return info