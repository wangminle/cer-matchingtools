"""
Jiebaåˆ†è¯å™¨å®ç°
åŸºäºjiebaåº“çš„åˆ†è¯å™¨ï¼Œå®Œå…¨å…¼å®¹ç°æœ‰åŠŸèƒ½
"""

from typing import List, Tuple
import jieba
import jieba.posseg
from .base import BaseTokenizer, TokenizerInitError, TokenizerProcessError


class JiebaTokenizer(BaseTokenizer):
    """
    Jiebaåˆ†è¯å™¨å®ç°
    åŸºäºjiebaåº“ï¼Œæä¾›ä¸­æ–‡åˆ†è¯ã€è¯æ€§æ ‡æ³¨å’Œç²¾ç¡®ä½ç½®åˆ†è¯åŠŸèƒ½
    """
    
    def __init__(self):
        super().__init__()
        self.name = "jieba"
    
    def initialize(self) -> bool:
        """
        åˆå§‹åŒ–Jiebaåˆ†è¯å™¨
        
        Returns:
            bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
            
        Raises:
            TokenizerInitError: åˆå§‹åŒ–å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            # Jiebaæ— éœ€ç‰¹æ®Šåˆå§‹åŒ–ï¼Œä½†å¯ä»¥é¢„åŠ è½½è¯å…¸
            # è¿™é‡Œè¿›è¡Œä¸€æ¬¡ç®€å•çš„åˆ†è¯æ“ä½œæ¥ç¡®ä¿jiebaæ­£å¸¸å·¥ä½œ
            test_result = list(jieba.cut("æµ‹è¯•"))
            if not test_result:
                raise TokenizerInitError("Jiebaåˆ†è¯å™¨æµ‹è¯•å¤±è´¥")
            
            # è·å–jiebaç‰ˆæœ¬ä¿¡æ¯
            try:
                # ğŸ”§ ä¼˜åŒ–: ä½¿ç”¨å¤šç§æ–¹å¼è·å–ç‰ˆæœ¬ä¿¡æ¯
                try:
                    from importlib.metadata import version
                    self.version = version('jieba')
                except ImportError:
                    # Python < 3.8 ä½¿ç”¨importlib_metadata
                    try:
                        from importlib_metadata import version
                        self.version = version('jieba')
                    except ImportError:
                        # å¤‡ç”¨æ–¹æ¡ˆï¼šä»æ¨¡å—å±æ€§è·å–
                        self.version = getattr(jieba, '__version__', 'unknown')
                except Exception as e:
                    print(f"è·å–Jiebaç‰ˆæœ¬å¤±è´¥: {str(e)}")
                    self.version = getattr(jieba, '__version__', 'unknown')
            except Exception:
                self.version = 'unknown'
            
            self.is_initialized = True
            return True
            
        except Exception as e:
            raise TokenizerInitError(f"Jiebaåˆ†è¯å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
    def cut(self, text: str) -> List[str]:
        """
        åŸºç¡€åˆ†è¯åŠŸèƒ½
        
        Args:
            text (str): å¾…åˆ†è¯çš„æ–‡æœ¬
            
        Returns:
            List[str]: åˆ†è¯ç»“æœåˆ—è¡¨
            
        Raises:
            TokenizerProcessError: åˆ†è¯å¤„ç†å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            cleaned_text = self.validate_text(text)
            if not cleaned_text:
                return []
            
            # ä½¿ç”¨jiebaè¿›è¡Œåˆ†è¯
            result = list(jieba.cut(cleaned_text))
            return result
            
        except Exception as e:
            raise TokenizerProcessError(f"Jiebaåˆ†è¯å¤±è´¥: {str(e)}")
    
    def posseg(self, text: str) -> List[Tuple[str, str]]:
        """
        è¯æ€§æ ‡æ³¨åŠŸèƒ½
        
        Args:
            text (str): å¾…æ ‡æ³¨çš„æ–‡æœ¬
            
        Returns:
            List[Tuple[str, str]]: (è¯è¯­, è¯æ€§)çš„å…ƒç»„åˆ—è¡¨
            
        Raises:
            TokenizerProcessError: è¯æ€§æ ‡æ³¨å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            cleaned_text = self.validate_text(text)
            if not cleaned_text:
                return []
            
            # ä½¿ç”¨jiebaè¿›è¡Œè¯æ€§æ ‡æ³¨
            result = []
            for word, flag in jieba.posseg.cut(cleaned_text):
                result.append((word, flag))
            
            return result
            
        except Exception as e:
            raise TokenizerProcessError(f"Jiebaè¯æ€§æ ‡æ³¨å¤±è´¥: {str(e)}")
    
    def tokenize(self, text: str) -> List[Tuple[str, int, int]]:
        """
        ç²¾ç¡®åˆ†è¯ï¼Œè¿”å›è¯è¯­åŠå…¶åœ¨åŸæ–‡ä¸­çš„ä½ç½®
        
        Args:
            text (str): å¾…åˆ†è¯çš„æ–‡æœ¬
            
        Returns:
            List[Tuple[str, int, int]]: (è¯è¯­, å¼€å§‹ä½ç½®, ç»“æŸä½ç½®)çš„å…ƒç»„åˆ—è¡¨
            
        Raises:
            TokenizerProcessError: åˆ†è¯å¤„ç†å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            cleaned_text = self.validate_text(text)
            if not cleaned_text:
                return []
            
            # ä½¿ç”¨jiebaçš„tokenizeåŠŸèƒ½è·å–ç²¾ç¡®ä½ç½®
            result = []
            for tk in jieba.tokenize(cleaned_text):
                word, start, end = tk
                result.append((word, start, end))
            
            return result
            
        except Exception as e:
            raise TokenizerProcessError(f"Jiebaç²¾ç¡®åˆ†è¯å¤±è´¥: {str(e)}")
    
    def get_info(self) -> dict:
        """
        è·å–Jiebaåˆ†è¯å™¨ä¿¡æ¯
        
        Returns:
            dict: åŒ…å«åˆ†è¯å™¨è¯¦ç»†ä¿¡æ¯çš„å­—å…¸
        """
        info = super().get_info()
        info.update({
            'description': 'åŸºäºjiebaåº“çš„ä¸­æ–‡åˆ†è¯å™¨',
            'features': ['åˆ†è¯', 'è¯æ€§æ ‡æ³¨', 'ç²¾ç¡®ä½ç½®åˆ†è¯'],
            'dependencies': ['jieba'],
            'performance': 'é«˜é€Ÿ',
            'accuracy': 'ä¸­ç­‰'
        })
        return info 